{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/ml/blob/master/notebooks/ml_machinelearninglandscape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/ml/blob/master/notebooks/ml_machinelearninglandscape.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesi√≥n 01: Proyecto Integral de Machine Learning\n",
    "## California Housing: De Datos Crudos a Modelo en Producci√≥n\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "**Profesor:** Marco Ter√°n  \n",
    "**Fecha:** 2025\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Tabla de Contenidos\n",
    "\n",
    "1. **Introducci√≥n y Objetivos** - Qu√© aprenderemos y por qu√© es importante\n",
    "2. **Obtenci√≥n de Datos** - C√≥mo conseguir y cargar datos\n",
    "3. **An√°lisis Exploratorio (EDA)** - Conociendo nuestros datos a fondo\n",
    "4. **Preparaci√≥n de Datos** - Limpieza y transformaci√≥n\n",
    "5. **Modelado** - Entrenando algoritmos de ML\n",
    "6. **Evaluaci√≥n** - Midiendo el rendimiento honestamente\n",
    "7. **Conclusiones** - Qu√© aprendimos y pr√≥ximos pasos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci√≥n y Objetivos <a name=\"intro\"></a>\n",
    "\n",
    "### ¬øQu√© vamos a construir hoy?\n",
    "\n",
    "Imagina que trabajas en una empresa inmobiliaria en California. Tu jefe te dice: \"Necesitamos una forma r√°pida y precisa de estimar el precio de las casas. Los tasadores humanos tardan d√≠as y cobran mucho. ¬øPuedes crear un sistema autom√°tico?\"\n",
    "\n",
    "**Este es exactamente el tipo de problema que Machine Learning puede resolver.**\n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar esta sesi√≥n, ser√°s capaz de:\n",
    "\n",
    "1. **Aplicar CRISP-DM** - La metodolog√≠a est√°ndar para proyectos de ciencia de datos\n",
    "2. **Realizar EDA exhaustivo** - Explorar datos como un detective buscando pistas\n",
    "3. **Preparar datos correctamente** - Limpiar, transformar y enriquecer informaci√≥n\n",
    "4. **Entrenar modelos de ML** - Desde los m√°s simples hasta Random Forests\n",
    "5. **Evaluar honestamente** - Sin trampas ni overfitting\n",
    "6. **Crear pipelines reproducibles** - C√≥digo profesional listo para producci√≥n\n",
    "\n",
    "### ¬øPor qu√© es importante este proyecto?\n",
    "\n",
    "- **Aplicaci√≥n real**: Miles de empresas necesitan predecir precios (casas, autos, productos)\n",
    "- **Conceptos fundamentales**: Todo lo que aprendas aqu√≠ se aplica a otros problemas\n",
    "- **Buenas pr√°cticas**: Aprender√°s a evitar los errores m√°s comunes en ML\n",
    "- **Portfolio**: Este proyecto demuestra habilidades valoradas en la industria\n",
    "\n",
    "### Lo que NO haremos (y por qu√©)\n",
    "\n",
    "- **No usaremos deep learning**: Para datos tabulares, m√©todos cl√°sicos suelen ser mejores\n",
    "- **No optimizaremos hasta el extremo**: El 80% del valor viene del 20% del esfuerzo\n",
    "- **No ignoraremos el negocio**: Un modelo preciso pero in√∫til no tiene valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"¬°Bienvenidos al primer notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Configuraci√≥n del Entorno\n",
    "\n",
    "### ¬øPor qu√© importan las versiones?\n",
    "\n",
    "En ML, la reproducibilidad es crucial. Imagina que tu modelo funciona perfectamente en tu computadora pero falla en producci√≥n. La causa m√°s com√∫n: diferentes versiones de librer√≠as.\n",
    "\n",
    "**Regla de oro**: Siempre documenta y verifica las versiones de tus dependencias.\n",
    "\n",
    "### Librer√≠as que usaremos\n",
    "\n",
    "- **NumPy**: El motor matem√°tico de Python. Maneja arrays y operaciones num√©ricas eficientemente\n",
    "- **Pandas**: Como Excel con superpoderes. Organiza datos en DataFrames (tablas)\n",
    "- **Matplotlib/Seaborn**: Nuestros artistas. Crean visualizaciones profesionales\n",
    "- **Scikit-learn**: La navaja suiza del ML. Contiene algoritmos, m√©tricas y utilidades\n",
    "\n",
    "### Configuraci√≥n visual\n",
    "\n",
    "Los defaults de matplotlib no son los m√°s bonitos. Vamos a configurar:\n",
    "- Estilo consistente para todos los gr√°ficos\n",
    "- Tama√±os legibles\n",
    "- Colores agradables\n",
    "- Formato de n√∫meros apropiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial del entorno\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar versi√≥n de Python\n",
    "assert sys.version_info >= (3, 7), \"Este notebook requiere Python 3.7 o superior\"\n",
    "\n",
    "print(f\"‚úÖ Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ‚â• 1.0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuraci√≥n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librer√≠as cr√≠ticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"‚úÖ scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ  Metodolog√≠a CRISP-DM\n",
    "\n",
    "### ¬øQu√© es CRISP-DM?\n",
    "\n",
    "**CRISP-DM** (Cross-Industry Standard Process for Data Mining) es el proceso est√°ndar que siguen las empresas para proyectos de datos. Fue creado en 1996 y sigue siendo el m√°s usado.\n",
    "\n",
    "### ¬øPor qu√© necesitamos una metodolog√≠a?\n",
    "\n",
    "Sin un proceso estructurado, es f√°cil:\n",
    "- Resolver el problema equivocado\n",
    "- Olvidar pasos importantes\n",
    "- Perder tiempo en callejones sin salida\n",
    "- No poder replicar resultados\n",
    "\n",
    "### Las 6 fases de CRISP-DM\n",
    "\n",
    "#### 1Ô∏è‚É£ Comprensi√≥n del Negocio (Business Understanding)\n",
    "\n",
    "**Qu√© hacemos**: Entender el problema REAL, no el que creemos que es.\n",
    "\n",
    "**Preguntas clave**:\n",
    "- ¬øQu√© decisi√≥n tomar√° el usuario con mi modelo?\n",
    "- ¬øCu√°nto vale resolver este problema? (ROI)\n",
    "- ¬øQu√© pasa si mi modelo se equivoca?\n",
    "- ¬øHay soluciones m√°s simples sin ML?\n",
    "\n",
    "**Ejemplo malo**: \"Quiero predecir precios de casas\"  \n",
    "**Ejemplo bueno**: \"Necesito estimar precios con error < $50k para que los agentes puedan dar cotizaciones r√°pidas que convenzan a los vendedores de listar con nosotros\"\n",
    "\n",
    "**Trampa com√∫n**: Saltar directo al modelado sin entender el contexto.\n",
    "\n",
    "#### 2Ô∏è‚É£ Comprensi√≥n de los Datos (Data Understanding)\n",
    "\n",
    "**Qu√© hacemos**: Explorar qu√© datos tenemos y qu√© calidad tienen.\n",
    "\n",
    "**Actividades**:\n",
    "- Recolectar datos de diversas fuentes\n",
    "- Explorar con estad√≠sticas descriptivas\n",
    "- Verificar calidad y completitud\n",
    "- Identificar problemas potenciales\n",
    "\n",
    "**Herramientas**: pandas.describe(), .info(), visualizaciones\n",
    "\n",
    "**Trampa com√∫n**: Asumir que los datos est√°n limpios y completos.\n",
    "\n",
    "#### 3Ô∏è‚É£ Preparaci√≥n de Datos (Data Preparation)\n",
    "\n",
    "**Qu√© hacemos**: Transformar datos crudos en formato apto para ML.\n",
    "\n",
    "**Tareas t√≠picas**:\n",
    "- Limpieza (valores faltantes, outliers)\n",
    "- Transformaci√≥n (normalizaci√≥n, encoding)\n",
    "- Creaci√≥n de features (ingenier√≠a de caracter√≠sticas)\n",
    "- Selecci√≥n de features relevantes\n",
    "\n",
    "**Regla 80/20**: Pasar√°s 80% del tiempo aqu√≠, 20% modelando.\n",
    "\n",
    "**Trampa com√∫n**: Data leakage (usar informaci√≥n del futuro o del conjunto de test).\n",
    "\n",
    "#### 4Ô∏è‚É£ Modelado (Modeling)\n",
    "\n",
    "**Qu√© hacemos**: Entrenar y ajustar algoritmos de ML.\n",
    "\n",
    "**Proceso**:\n",
    "1. Seleccionar algoritmos candidatos\n",
    "2. Entrenar con datos de entrenamiento\n",
    "3. Ajustar hiperpar√°metros\n",
    "4. Validar con cross-validation\n",
    "\n",
    "**Importante**: M√°s complejo ‚â† Mejor. Empieza simple.\n",
    "\n",
    "**Trampa com√∫n**: Overfitting (memorizar en lugar de aprender).\n",
    "\n",
    "#### 5Ô∏è‚É£ Evaluaci√≥n (Evaluation)\n",
    "\n",
    "**Qu√© hacemos**: Verificar si el modelo cumple los objetivos de negocio.\n",
    "\n",
    "**No es solo accuracy**:\n",
    "- ¬øResuelve el problema de negocio?\n",
    "- ¬øEs lo suficientemente r√°pido?\n",
    "- ¬øEs interpretable si es necesario?\n",
    "- ¬øFunciona en todos los segmentos importantes?\n",
    "\n",
    "**Trampa com√∫n**: Optimizar la m√©trica equivocada.\n",
    "\n",
    "#### 6Ô∏è‚É£ Despliegue (Deployment)\n",
    "\n",
    "**Qu√© hacemos**: Poner el modelo en producci√≥n.\n",
    "\n",
    "**Consideraciones**:\n",
    "- Integraci√≥n con sistemas existentes\n",
    "- Monitoreo de performance\n",
    "- Plan de actualizaci√≥n/reentrenamiento\n",
    "- Documentaci√≥n y handover\n",
    "\n",
    "**Realidad**: Un modelo que no se usa no genera valor.\n",
    "\n",
    "### El secreto: Es ITERATIVO, no lineal\n",
    "\n",
    "CRISP-DM no es una cascada, es un ciclo. Constantemente volvemos atr√°s cuando descubrimos nuevos insights.\n",
    "\n",
    "---\n",
    "\n",
    "## üíº 3. Comprensi√≥n del Negocio\n",
    "\n",
    "### El problema de California Housing Corp\n",
    "\n",
    "**Contexto**: Es 1990. California Housing Corp maneja miles de propiedades. Los tasadores est√°n sobrecargados y los clientes se quejan de la lentitud.\n",
    "\n",
    "**Problema actual**:\n",
    "- Tasaci√≥n manual toma 2-3 d√≠as\n",
    "- Costo: $500 por tasaci√≥n\n",
    "- Inconsistencia entre tasadores (subjetividad)\n",
    "- Cuellos de botella en temporada alta\n",
    "\n",
    "**Soluci√≥n propuesta**: Sistema autom√°tico de predicci√≥n de precios\n",
    "\n",
    "### Definiendo el √©xito\n",
    "\n",
    "* **M√©trica de negocio**: Reducir tiempo de respuesta de 3 d√≠as a 3 segundos\n",
    "\n",
    "* **M√©trica t√©cnica**: Error Absoluto Medio (MAE) < $50,000\n",
    "\n",
    "* **¬øPor qu√© $50,000?**\n",
    "    - Precio medio en California: ~200,000\n",
    "    - Error del 25% es aceptable para cotizaci√≥n inicial\n",
    "    - Tasadores humanos tienen error similar\n",
    "\n",
    "### Preguntas cr√≠ticas antes de empezar\n",
    "\n",
    "**1. ¬øRealmente necesitamos ML?**\n",
    "- Alternativa 1: Precio promedio del barrio ‚Üí Muy impreciso\n",
    "- Alternativa 2: Reglas simples ‚Üí No captura complejidad\n",
    "- Conclusi√≥n: S√≠, ML es apropiado\n",
    "\n",
    "**2. ¬øQu√© pasa si el modelo falla?**\n",
    "- Plan B: Siempre tener tasador de respaldo\n",
    "- Transparencia: Decir que es estimaci√≥n autom√°tica\n",
    "- Rangos: Dar intervalo de confianza, no solo un n√∫mero\n",
    "\n",
    "**3. ¬øC√≥mo mediremos el impacto?**\n",
    "- Velocidad de respuesta a clientes\n",
    "- Tasa de conversi√≥n (cotizaci√≥n ‚Üí venta)\n",
    "- Satisfacci√≥n del cliente\n",
    "- Ahorro en costos de tasaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del proceso CRISP-DM\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Definir las fases\n",
    "phases = [\n",
    "    \"1. Comprensi√≥n\\ndel Negocio\",\n",
    "    \"2. Comprensi√≥n\\nde los Datos\", \n",
    "    \"3. Preparaci√≥n\\nde Datos\",\n",
    "    \"4. Modelado\",\n",
    "    \"5. Evaluaci√≥n\",\n",
    "    \"6. Despliegue\"\n",
    "]\n",
    "\n",
    "# Posiciones en c√≠rculo\n",
    "angles = np.linspace(0, 2*np.pi, len(phases), endpoint=False)\n",
    "x = np.cos(angles)\n",
    "y = np.sin(angles)\n",
    "\n",
    "# Dibujar el ciclo\n",
    "for i in range(len(phases)):\n",
    "    circle = plt.Circle((x[i]*3, y[i]*3), 0.8, color=f'C{i}', alpha=0.7)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x[i]*3, y[i]*3, phases[i], ha='center', va='center', \n",
    "            fontsize=11, fontweight='bold', color='white')\n",
    "    \n",
    "    # Flechas de conexi√≥n\n",
    "    next_i = (i + 1) % len(phases)\n",
    "    ax.annotate('', xy=(x[next_i]*2.3, y[next_i]*2.3), \n",
    "                xytext=(x[i]*3.7, y[i]*3.7),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "ax.text(0, 0, 'CRISP-DM', fontsize=16, fontweight='bold', ha='center')\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.axis('off')\n",
    "plt.title(\"Proceso Iterativo CRISP-DM\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Obtenci√≥n y Comprensi√≥n de Datos <a name=\"datos\"></a>\n",
    "\n",
    "### El dataset de California Housing\n",
    "\n",
    "**Origen**: Censo de California de 1990\n",
    "**Tama√±o**: 20,640 distritos\n",
    "**Granularidad**: Cada fila es un distrito, no una casa individual\n",
    "\n",
    "### ¬øPor qu√© este dataset?\n",
    "\n",
    "- **Cl√°sico en ML**: Bien estudiado, podemos comparar resultados\n",
    "- **Tama√±o apropiado**: Ni muy peque√±o ni muy grande para aprender\n",
    "- **Problemas reales**: Tiene valores faltantes y peculiaridades\n",
    "- **M√∫ltiples tipos de datos**: Num√©ricos y categ√≥ricos\n",
    "\n",
    "### Estrategia de descarga robusta\n",
    "\n",
    "Implementaremos:\n",
    "1. **Cach√© local**: Si ya descargamos, no repetir\n",
    "2. **Manejo de errores**: Si falla la descarga, informar claramente\n",
    "3. **Estructura organizada**: Carpeta datasets/ para todos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n mejorada para descargar datos\n",
    "def load_housing_data():\n",
    "    \"\"\"\n",
    "    Descarga y carga el dataset de California Housing.\n",
    "    Incluye manejo de errores y cach√© local.\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    data_path = Path(\"datasets\")\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    tarball_path = data_path / \"housing.tgz\"\n",
    "    csv_path = data_path / \"housing\" / \"housing.csv\"\n",
    "    \n",
    "    # Verificar si ya existe el CSV\n",
    "    if csv_path.is_file():\n",
    "        print(\"üìÅ Cargando datos desde cach√© local...\")\n",
    "        return pd.read_csv(csv_path)\n",
    "    \n",
    "    # Descargar si no existe\n",
    "    if not tarball_path.is_file():\n",
    "        print(\"üì• Descargando dataset...\")\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, tarball_path)\n",
    "            print(\"‚úÖ Descarga completada\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en descarga: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Extraer archivo\n",
    "    print(\"üì¶ Extrayendo archivos...\")\n",
    "    with tarfile.open(tarball_path) as housing_tarball:\n",
    "        housing_tarball.extractall(path=data_path)\n",
    "    \n",
    "    print(\"‚úÖ Datos cargados exitosamente\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Cargar datos\n",
    "housing = load_housing_data()\n",
    "print(f\"\\nüìä Dataset cargado: {housing.shape[0]:,} filas √ó {housing.shape[1]} columnas\")\n",
    "\n",
    "# For Kaggle environment\n",
    "#housing = pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### ¬øQu√© es EDA y por qu√© es crucial?\n",
    "\n",
    "**EDA es como ser un detective**: Buscas pistas, anomal√≠as y patrones en los datos.\n",
    "\n",
    "**John Tukey** (inventor del EDA) dijo: \"Es mejor una respuesta aproximada a la pregunta correcta que una respuesta exacta a la pregunta incorrecta.\"\n",
    "\n",
    "### Primera impresi√≥n: Vista r√°pida\n",
    "\n",
    "**¬øQu√© buscamos?**\n",
    "- Tipos de datos (num√©ricos, texto, fechas)\n",
    "- Dimensiones (filas √ó columnas)\n",
    "- Valores faltantes obvios\n",
    "- Rangos sospechosos\n",
    "\n",
    "**Herramientas**: head(), info(), describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera Inspecci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista general del dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Mostrar primeras filas con formato mejorado\n",
    "display(housing.head().style.background_gradient(cmap='coolwarm', subset=['median_house_value']))\n",
    "\n",
    "# Informaci√≥n detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "housing.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(housing.describe().round(2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendiendo cada variable\n",
    "\n",
    "**Variables geogr√°ficas**:\n",
    "- `longitude`, `latitude`: Coordenadas GPS\n",
    "- Nos permiten visualizar en mapa\n",
    "- Pueden revelar patrones espaciales (precios por zona)\n",
    "\n",
    "**Variables demogr√°ficas**:\n",
    "- `population`: Total de personas en el distrito\n",
    "- `households`: N√∫mero de hogares (familias)\n",
    "- `total_rooms`, `total_bedrooms`: Infraestructura habitacional\n",
    "\n",
    "**Variables econ√≥micas**:\n",
    "- `median_income`: Ingreso mediano (en decenas de miles)\n",
    "- `median_house_value`: **NUESTRA VARIABLE OBJETIVO**\n",
    "\n",
    "**Variable temporal**:\n",
    "- `housing_median_age`: Edad de las construcciones\n",
    "\n",
    "**Variable categ√≥rica**:\n",
    "- `ocean_proximity`: Relaci√≥n con el oc√©ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de metadatos\n",
    "metadata = {\n",
    "    'Variable': ['longitude', 'latitude', 'housing_median_age', 'total_rooms', \n",
    "                 'total_bedrooms', 'population', 'households', 'median_income', \n",
    "                 'median_house_value', 'ocean_proximity'],\n",
    "    'Tipo': ['Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica', \n",
    "             'Num√©rica', 'Num√©rica', 'Num√©rica', 'Num√©rica (Target)', 'Categ√≥rica'],\n",
    "    'Descripci√≥n': [\n",
    "        'Longitud geogr√°fica (m√°s oeste = mayor valor)',\n",
    "        'Latitud geogr√°fica (m√°s norte = mayor valor)',\n",
    "        'Edad mediana de las casas en el distrito (a√±os)',\n",
    "        'N√∫mero total de habitaciones en el distrito',\n",
    "        'N√∫mero total de dormitorios en el distrito',\n",
    "        'Poblaci√≥n total del distrito',\n",
    "        'N√∫mero total de hogares en el distrito',\n",
    "        'Ingreso mediano de los hogares (√ó$10,000)',\n",
    "        'üéØ Valor mediano de las casas (USD)',\n",
    "        'Proximidad al oc√©ano'\n",
    "    ],\n",
    "    'Valores Faltantes': [\n",
    "        housing['longitude'].isnull().sum(),\n",
    "        housing['latitude'].isnull().sum(),\n",
    "        housing['housing_median_age'].isnull().sum(),\n",
    "        housing['total_rooms'].isnull().sum(),\n",
    "        housing['total_bedrooms'].isnull().sum(),\n",
    "        housing['population'].isnull().sum(),\n",
    "        housing['households'].isnull().sum(),\n",
    "        housing['median_income'].isnull().sum(),\n",
    "        housing['median_house_value'].isnull().sum(),\n",
    "        housing['ocean_proximity'].isnull().sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "display(df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if x > 0 else '', \n",
    "    subset=['Valores Faltantes']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Detectando problemas en los datos\n",
    "\n",
    "#### An√°lisis de Valores Faltantes (Missing values)\n",
    "\n",
    "**¬øPor qu√© faltan datos?**\n",
    "- No se recopil√≥ (olvido, opcional)\n",
    "- Error en recopilaci√≥n\n",
    "- No aplica (ej: \"n√∫mero de hijos\" para solteros)\n",
    "\n",
    "**Estrategias**:\n",
    "1. **Eliminar filas**: Si son pocas (<5%)\n",
    "2. **Eliminar columna**: Si falta mucho (>60%)\n",
    "3. **Imputar**: Rellenar con media/mediana/moda\n",
    "4. **Indicador**: Crear variable \"era_faltante\"\n",
    "\n",
    "**En nuestro caso**: total_bedrooms falta en 207 distritos (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado de valores faltantes\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"An√°lisis completo de valores faltantes\"\"\"\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': df.isnull().sum(),\n",
    "        'Porcentaje': (df.isnull().sum() / len(df)) * 100,\n",
    "        'Tipo_Dato': df.dtypes\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values(\n",
    "        'Porcentaje', ascending=False\n",
    "    )\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        # Visualizaci√≥n\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "        ax1.set_xlabel('Columna')\n",
    "        ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "        ax1.set_title('Valores Faltantes por Columna')\n",
    "        ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Heatmap de patrones\n",
    "        import seaborn as sns\n",
    "        msno_data = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "        sns.heatmap(msno_data.corr(), annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                   ax=ax2, vmin=-1, vmax=1)\n",
    "        ax2.set_title('Correlaci√≥n de Patrones de Valores Faltantes')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return missing_df\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores faltantes en el dataset\")\n",
    "        return None\n",
    "\n",
    "missing_analysis = analyze_missing_values(housing)\n",
    "if missing_analysis is not None:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estad√≠sticas descriptivas: Los n√∫meros cuentan historias\n",
    "\n",
    "**¬øQu√© nos dicen las estad√≠sticas?**\n",
    "\n",
    "**Media vs Mediana**:\n",
    "- Si media > mediana: Sesgo a la derecha (valores extremos altos)\n",
    "- Si media < mediana: Sesgo a la izquierda (valores extremos bajos)\n",
    "- Si media ‚âà mediana: Distribuci√≥n sim√©trica\n",
    "\n",
    "**Desviaci√≥n est√°ndar**:\n",
    "- Alta: Mucha variabilidad (cuidado con outliers)\n",
    "- Baja: Datos concentrados (posible poca informaci√≥n)\n",
    "\n",
    "**Min/Max sospechosos**:\n",
    "- Edad m√°xima = 52: ¬øCensura de datos?\n",
    "- Precio m√°ximo = $500,001: Definitivamente censura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis Univariado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¬øQu√© buscamos en un histograma?**\n",
    "\n",
    "1. **Forma de campana** (normal): Ideal para muchos algoritmos\n",
    "2. **Sesgo** (cola larga): Considerar transformaci√≥n logar√≠tmica\n",
    "3. **Bimodal** (dos jorobas): Posibles subgrupos diferentes\n",
    "4. **Uniforme** (plano): Poca informaci√≥n predictiva\n",
    "5. **Picos extra√±os**: Valores artificiales o errores\n",
    "\n",
    "**Ejemplo**: median_house_value tiene pico en $500k ‚Üí Censura de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para an√°lisis univariado robusto\n",
    "def univariate_analysis(df, column, target=None):\n",
    "    \"\"\"An√°lisis univariado con estad√≠sticas robustas\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Histograma con KDE\n",
    "    ax1 = axes[0, 0]\n",
    "    df[column].hist(bins=50, edgecolor='black', alpha=0.7, ax=ax1)\n",
    "    ax1.axvline(df[column].mean(), color='red', linestyle='--', label=f'Media: {df[column].mean():.2f}')\n",
    "    ax1.axvline(df[column].median(), color='green', linestyle='--', label=f'Mediana: {df[column].median():.2f}')\n",
    "    ax1.set_title(f'Distribuci√≥n de {column}')\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Boxplot\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(df[column].dropna(), vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    ax2.set_title(f'Boxplot de {column}')\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Detectar outliers\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[column] < Q1 - 1.5 * IQR) | (df[column] > Q3 + 1.5 * IQR)]\n",
    "    ax2.text(1.1, Q3, f'Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)', \n",
    "             fontsize=10)\n",
    "    \n",
    "    # 3. Q-Q Plot\n",
    "    ax3 = axes[1, 0]\n",
    "    from scipy import stats\n",
    "    stats.probplot(df[column].dropna(), dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Relaci√≥n con target (si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns:\n",
    "        ax4.scatter(df[column], df[target], alpha=0.5, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f'{column} vs {target}')\n",
    "        \n",
    "        # Agregar l√≠nea de tendencia\n",
    "        z = np.polyfit(df[column].dropna(), df[target][df[column].notna()], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax4.plot(df[column].sort_values(), p(df[column].sort_values()), \n",
    "                \"r--\", alpha=0.8, label=f'Tendencia')\n",
    "        \n",
    "        # Calcular correlaci√≥n\n",
    "        corr = df[column].corr(df[target])\n",
    "        ax4.text(0.05, 0.95, f'Correlaci√≥n: {corr:.3f}', \n",
    "                transform=ax4.transAxes, fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "        ax4.legend()\n",
    "    else:\n",
    "        # Estad√≠sticas adicionales\n",
    "        ax4.axis('off')\n",
    "        stats_text = f\"\"\"\n",
    "        Estad√≠sticas Robustas:\n",
    "        \n",
    "        ‚Ä¢ Media: {df[column].mean():.2f}\n",
    "        ‚Ä¢ Mediana: {df[column].median():.2f}\n",
    "        ‚Ä¢ Desv. Est√°ndar: {df[column].std():.2f}\n",
    "        ‚Ä¢ MAD: {stats.median_abs_deviation(df[column].dropna()):.2f}\n",
    "        ‚Ä¢ Asimetr√≠a: {df[column].skew():.2f}\n",
    "        ‚Ä¢ Curtosis: {df[column].kurtosis():.2f}\n",
    "        ‚Ä¢ Rango: [{df[column].min():.2f}, {df[column].max():.2f}]\n",
    "        ‚Ä¢ IQR: {IQR:.2f}\n",
    "        ‚Ä¢ CV: {df[column].std()/df[column].mean():.2f}\n",
    "        \"\"\"\n",
    "        ax4.text(0.1, 0.5, stats_text, transform=ax4.transAxes, \n",
    "                fontsize=11, verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "    \n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'An√°lisis Univariado: {column}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analizar variables num√©ricas clave\n",
    "for col in ['median_income', 'housing_median_age', 'median_house_value']:\n",
    "    univariate_analysis(housing, col, 'median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Variable Categ√≥rica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de ocean_proximity\n",
    "def analyze_categorical(df, cat_col, target_col):\n",
    "    \"\"\"An√°lisis completo de variable categ√≥rica\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Distribuci√≥n de categor√≠as\n",
    "    ax1 = axes[0, 0]\n",
    "    counts = df[cat_col].value_counts()\n",
    "    ax1.bar(counts.index, counts.values, color=plt.cm.Set3(range(len(counts))))\n",
    "    ax1.set_title(f'Distribuci√≥n de {cat_col}')\n",
    "    ax1.set_xlabel(cat_col)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Agregar porcentajes\n",
    "    for i, (idx, val) in enumerate(counts.items()):\n",
    "        ax1.text(i, val, f'{val}\\n({val/len(df)*100:.1f}%)', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Pie chart\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(counts.values, labels=counts.index, autopct='%1.1f%%',\n",
    "            colors=plt.cm.Set3(range(len(counts))))\n",
    "    ax2.set_title(f'Proporci√≥n de {cat_col}')\n",
    "    \n",
    "    # 3. Boxplot por categor√≠a\n",
    "    ax3 = axes[1, 0]\n",
    "    df.boxplot(column=target_col, by=cat_col, ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col}')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 4. Estad√≠sticas por categor√≠a\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    stats_by_cat = df.groupby(cat_col)[target_col].agg([\n",
    "        'count', 'mean', 'median', 'std'\n",
    "    ]).round(2)\n",
    "    \n",
    "    table_data = []\n",
    "    for idx, row in stats_by_cat.iterrows():\n",
    "        table_data.append([idx, f\"{row['count']:.0f}\", \n",
    "                          f\"${row['mean']:,.0f}\", \n",
    "                          f\"${row['median']:,.0f}\",\n",
    "                          f\"${row['std']:,.0f}\"])\n",
    "    \n",
    "    table = ax4.table(cellText=table_data,\n",
    "                     colLabels=['Categor√≠a', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.3, 0.15, 0.2, 0.2, 0.2])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Colorear encabezados\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "    \n",
    "    plt.suptitle(f'An√°lisis de Variable Categ√≥rica: {cat_col}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_categorical(housing, 'ocean_proximity', 'median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis Geoespacial\n",
    "\n",
    "**Mapas geogr√°ficos:** Location, location, location\n",
    "\n",
    "**¬øPor qu√© graficar geogr√°ficamente?**\n",
    "- Precios inmobiliarios son altamente locales\n",
    "- Revelamos clusters (Silicon Valley, LA, San Diego)\n",
    "- Detectamos anomal√≠as geogr√°ficas\n",
    "\n",
    "**T√©cnicas**:\n",
    "- Scatter plot simple: Ver forma de California\n",
    "- Color por precio: Zonas caras vs baratas\n",
    "- Tama√±o por poblaci√≥n: Densidad urbana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n geogr√°fica mejorada\n",
    "def plot_geographical_data(df):\n",
    "    \"\"\"Visualizaci√≥n geogr√°fica de California con precios\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    \n",
    "    # 1. Mapa de densidad\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(df['longitude'], df['latitude'], alpha=0.1, s=1, c='blue')\n",
    "    ax1.set_xlabel('Longitud')\n",
    "    ax1.set_ylabel('Latitud')\n",
    "    ax1.set_title('Densidad de Puntos de Datos')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Mapa de precios\n",
    "    ax2 = axes[1]\n",
    "    scatter = ax2.scatter(df['longitude'], df['latitude'], \n",
    "                         c=df['median_house_value'], cmap='YlOrRd',\n",
    "                         s=df['population']/100, alpha=0.4)\n",
    "    ax2.set_xlabel('Longitud')\n",
    "    ax2.set_ylabel('Latitud')\n",
    "    ax2.set_title('Precio Medio de Vivienda por Ubicaci√≥n\\n(Tama√±o = Poblaci√≥n)')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Precio Medio ($)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Mapa de ingresos\n",
    "    ax3 = axes[2]\n",
    "    scatter2 = ax3.scatter(df['longitude'], df['latitude'],\n",
    "                          c=df['median_income'], cmap='viridis',\n",
    "                          s=20, alpha=0.4)\n",
    "    ax3.set_xlabel('Longitud')\n",
    "    ax3.set_ylabel('Latitud')\n",
    "    ax3.set_title('Ingreso Medio por Ubicaci√≥n')\n",
    "    plt.colorbar(scatter2, ax=ax3, label='Ingreso Medio (√ó$10k)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Identificar zonas de alto valor\n",
    "    high_value = df[df['median_house_value'] > df['median_house_value'].quantile(0.9)]\n",
    "    for ax in axes[1:]:\n",
    "        ax.scatter(high_value['longitude'], high_value['latitude'],\n",
    "                  color='red', s=100, alpha=0.5, marker='*',\n",
    "                  label='Top 10% Precio')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('An√°lisis Geoespacial de California Housing', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estad√≠sticas por regi√≥n\n",
    "    print(\"\\nüìç Estad√≠sticas por Proximidad al Oc√©ano:\")\n",
    "    print(\"=\" * 60)\n",
    "    stats = df.groupby('ocean_proximity').agg({\n",
    "        'median_house_value': ['mean', 'median', 'std'],\n",
    "        'median_income': 'mean',\n",
    "        'population': 'sum'\n",
    "    }).round(2)\n",
    "    display(stats)\n",
    "\n",
    "plot_geographical_data(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de correlaci√≥n: Relaciones entre variables\n",
    "\n",
    "**Correlaci√≥n de Pearson**:\n",
    "- Mide relaci√≥n **lineal** entre variables\n",
    "- Rango: [-1, +1]\n",
    "- 0 = Sin relaci√≥n lineal (¬°pero puede haber no-lineal!)\n",
    "\n",
    "**Interpretaci√≥n**:\n",
    "- |r| < 0.1: Muy d√©bil\n",
    "- 0.1 ‚â§ |r| < 0.3: D√©bil\n",
    "- 0.3 ‚â§ |r| < 0.5: Moderada\n",
    "- 0.5 ‚â§ |r| < 0.7: Fuerte\n",
    "- |r| ‚â• 0.7: Muy fuerte\n",
    "\n",
    "**Cuidado**: Correlaci√≥n ‚â† Causalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaci√≥n mejorado\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"An√°lisis de correlaci√≥n con m√∫ltiples m√©tricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlaci√≥n de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlaci√≥n de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlaci√≥n de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlaci√≥n de Spearman (Monot√≥nica)')\n",
    "    \n",
    "    # 3. Correlaci√≥n con variable objetivo\n",
    "    target_corr = df[numeric_cols].corr()['median_house_value'].sort_values(ascending=False)\n",
    "    colors = ['green' if x > 0 else 'red' for x in target_corr.values]\n",
    "    target_corr.plot(kind='barh', ax=axes[2], color=colors)\n",
    "    axes[2].set_title('Correlaci√≥n con Precio de Vivienda')\n",
    "    axes[2].set_xlabel('Coeficiente de Correlaci√≥n')\n",
    "    axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Correlaci√≥n Multi-m√©trica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\nüîó Correlaciones Significativas con el Precio:\")\n",
    "    print(\"=\" * 50)\n",
    "    significant_corr = target_corr[abs(target_corr) > 0.1].drop('median_house_value')\n",
    "    for var, corr in significant_corr.items():\n",
    "        strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"D√©bil\"\n",
    "        direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "        print(f\"  ‚Ä¢ {var:20s}: {corr:+.3f} ({strength} {direction})\")\n",
    "\n",
    "correlation_analysis(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecci√≥n de Anomal√≠as y Outliers\n",
    "\n",
    "**Outliers:** ¬øErrores o informaci√≥n valiosa?\n",
    "\n",
    "**Tipos de outliers**:\n",
    "1. **Errores**: Edad = 999 a√±os ‚Üí Eliminar\n",
    "2. **Casos raros pero v√°lidos**: Mansi√≥n de $50M ‚Üí Mantener\n",
    "3. **Diferentes poblaciones**: Empresa en zona residencial ‚Üí Investigar\n",
    "\n",
    "**M√©todos de detecci√≥n**:\n",
    "- **IQR**: Fuera de Q1-1.5√óIQR o Q3+1.5√óIQR\n",
    "- **Z-score**: |z| > 3\n",
    "- **Isolation Forest**: Algoritmo de ML para anomal√≠as\n",
    "\n",
    "#### Distribuciones problem√°ticas\n",
    "\n",
    "**Alta asimetr√≠a (skewness)**:\n",
    "- Problema: Muchos algoritmos asumen normalidad\n",
    "- Soluci√≥n: Transformaci√≥n log, sqrt o Box-Cox\n",
    "\n",
    "**Alta curtosis**:\n",
    "- Problema: Colas pesadas, muchos outliers\n",
    "- Soluci√≥n: Winsorization (cap de valores extremos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detecci√≥n de outliers usando m√∫ltiples m√©todos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # M√©todo 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # M√©todo 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # M√©todo 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (M√©todo IQR)')\n",
    "    ax1.set_ylabel('N√∫mero de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribuci√≥n de outliers por m√©todo\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparaci√≥n de M√©todos de Detecci√≥n')\n",
    "    ax2.set_ylabel('N√∫mero de Outliers Detectados')\n",
    "    ax2.set_xlabel('M√©todo')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estad√≠stico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detecci√≥n de Anomal√≠as:\n",
    "    \n",
    "    ‚Ä¢ Total de observaciones: {len(df):,}\n",
    "    ‚Ä¢ Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables m√°s afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Recomendaci√≥n: Investigar outliers antes de eliminar.\n",
    "    Pueden contener informaci√≥n valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Outliers y Anomal√≠as', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparaci√≥n de Datos <a name=\"prep\"></a>\n",
    "\n",
    "### Divisi√≥n Train/Test: La regla de oro del ML\n",
    "\n",
    "**¬øPor qu√© dividir los datos?**\n",
    "\n",
    "Imagina estudiar para un examen teniendo las preguntas y respuestas exactas. ¬øAprobar√≠as? S√≠. ¬øAprendiste? No.\n",
    "\n",
    "Lo mismo pasa en ML: Si evaluamos con los mismos datos que usamos para entrenar, el modelo puede memorizar en lugar de aprender patrones.\n",
    "\n",
    "**La divisi√≥n t√≠pica**:\n",
    "- **Training set (60-80%)**: Para entrenar el modelo\n",
    "- **Validation set (10-20%)**: Para ajustar hiperpar√°metros\n",
    "- **Test set (10-20%)**: Para evaluaci√≥n final\n",
    "\n",
    "**Regla de oro**: NUNCA uses el test set hasta el final. Es tu examen final.\n",
    "\n",
    "### Divisi√≥n aleatoria vs estratificada\n",
    "\n",
    "**Divisi√≥n aleatoria**:\n",
    "- Simple: Tomar muestras al azar\n",
    "- Problema: Puede no representar bien subgrupos peque√±os\n",
    "\n",
    "**Divisi√≥n estratificada**:\n",
    "- Mantiene proporciones de grupos importantes\n",
    "- Ejemplo: Mismo % de casas caras/baratas en train y test\n",
    "- Crucial cuando hay desbalance de clases\n",
    "\n",
    "**¬øCu√°ndo usar estratificada?**\n",
    "- Clasificaci√≥n con clases desbalanceadas\n",
    "- Cuando una variable es cr√≠tica para el negocio\n",
    "- Datasets peque√±os donde el azar puede sesgar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "def create_train_test_split(df, test_size=0.2, stratify_column=None):\n",
    "    \"\"\"\n",
    "    Crea conjuntos de entrenamiento y prueba con estratificaci√≥n opcional.\n",
    "    \"\"\"\n",
    "    if stratify_column:\n",
    "        # Crear bins para estratificaci√≥n\n",
    "        df['stratify_cat'] = pd.cut(df[stratify_column],\n",
    "                                    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                                    labels=[1, 2, 3, 4, 5])\n",
    "        \n",
    "        # Divisi√≥n estratificada\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "        for train_idx, test_idx in splitter.split(df, df['stratify_cat']):\n",
    "            train_set = df.iloc[train_idx].copy()\n",
    "            test_set = df.iloc[test_idx].copy()\n",
    "        \n",
    "        # Verificar proporciones\n",
    "        print(\"üìä Verificaci√≥n de Estratificaci√≥n:\")\n",
    "        print(\"=\" * 50)\n",
    "        original_props = df['stratify_cat'].value_counts(normalize=True).sort_index()\n",
    "        train_props = train_set['stratify_cat'].value_counts(normalize=True).sort_index()\n",
    "        test_props = test_set['stratify_cat'].value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            'Original': original_props,\n",
    "            'Train': train_props,\n",
    "            'Test': test_props\n",
    "        })\n",
    "        comparison['Train_Error_%'] = (comparison['Train'] / comparison['Original'] - 1) * 100\n",
    "        comparison['Test_Error_%'] = (comparison['Test'] / comparison['Original'] - 1) * 100\n",
    "        display(comparison.round(2))\n",
    "        \n",
    "        # Eliminar columna temporal\n",
    "        for set_ in (train_set, test_set):\n",
    "            set_.drop('stratify_cat', axis=1, inplace=True)\n",
    "    else:\n",
    "        train_set, test_set = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Conjuntos creados:\")\n",
    "    print(f\"   ‚Ä¢ Entrenamiento: {len(train_set):,} muestras ({len(train_set)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Prueba: {len(test_set):,} muestras ({len(test_set)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "# Crear conjuntos con estratificaci√≥n por ingreso medio\n",
    "strat_train_set, strat_test_set = create_train_test_split(\n",
    "    housing, test_size=0.2, stratify_column='median_income'\n",
    ")\n",
    "\n",
    "# Hacer copia para trabajar\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingenier√≠a de caracter√≠sticas: El arte del ML\n",
    "\n",
    "**\"Feature engineering is the art of making data useful\"**\n",
    "\n",
    "#### ¬øPor qu√© crear nuevas caracter√≠sticas?\n",
    "\n",
    "Los modelos de ML solo pueden encontrar patrones en los datos que les das. Si les das mejores representaciones, encuentran mejores patrones.\n",
    "\n",
    "#### Tipos de nuevas caracter√≠sticas\n",
    "\n",
    "**1. Ratios y proporciones**:\n",
    "- `rooms_per_household`: Tama√±o promedio de casa\n",
    "- `population_per_household`: Tama√±o de familia\n",
    "- `bedrooms_ratio`: Proporci√≥n dormitorios/habitaciones\n",
    "\n",
    "**¬øPor qu√© funcionan?** Los totales dependen del tama√±o del distrito, los ratios no.\n",
    "\n",
    "**2. Combinaciones**:\n",
    "- `location_score = latitude √ó longitude`: Interacci√≥n geogr√°fica\n",
    "- `income_per_room = median_income / rooms_per_household`\n",
    "\n",
    "**3. Transformaciones**:\n",
    "- `log_population`: Para suavizar distribuciones sesgadas\n",
    "- `age_squared`: Para capturar relaciones no lineales\n",
    "\n",
    "**4. Binning (categorizaci√≥n)**:\n",
    "- Edad: Nueva ‚Üí Media ‚Üí Antigua\n",
    "- Ingreso: Bajo ‚Üí Medio ‚Üí Alto\n",
    "\n",
    "**5. Informaci√≥n de dominio**:\n",
    "- Distancia a ciudad principal\n",
    "- Distancia a escuelas/hospitales\n",
    "- Zona s√≠smica\n",
    "\n",
    "#### Buenas pr√°cticas en feature engineering\n",
    "\n",
    "‚úÖ **DO**:\n",
    "- Piensa como un experto del dominio\n",
    "- Valida que mejoran el modelo\n",
    "- Documenta la l√≥gica de cada feature\n",
    "- Mant√©n interpretabilidad si es importante\n",
    "\n",
    "‚ùå **DON'T**:\n",
    "- Crear cientos de features sin sentido\n",
    "- Usar informaci√≥n del futuro (data leakage)\n",
    "- Complicar innecesariamente\n",
    "- Olvidar que \"m√°s features ‚â† mejor modelo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Crea nuevas caracter√≠sticas basadas en conocimiento del dominio.\n",
    "    \"\"\"\n",
    "    print(\"üîß Creando nuevas caracter√≠sticas...\")\n",
    "    \n",
    "    # Caracter√≠sticas por hogar\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "    \n",
    "    # Caracter√≠sticas geogr√°ficas\n",
    "    df['location_index'] = df['latitude'] * df['longitude']  # √çndice de ubicaci√≥n simple\n",
    "    \n",
    "    # Caracter√≠sticas de densidad\n",
    "    df['housing_density'] = df['households'] / df['population']\n",
    "    \n",
    "    # Log transformations para variables sesgadas\n",
    "    df['log_median_income'] = np.log1p(df['median_income'])\n",
    "    df['log_population'] = np.log1p(df['population'])\n",
    "    \n",
    "    # Binning de edad de vivienda\n",
    "    df['age_category'] = pd.cut(df['housing_median_age'], \n",
    "                                bins=[0, 10, 20, 30, 40, np.inf],\n",
    "                                labels=['0-10', '11-20', '21-30', '31-40', '40+'])\n",
    "    \n",
    "    print(f\"‚úÖ Nuevas caracter√≠sticas creadas: {len(df.columns) - len(housing.columns)}\")\n",
    "    \n",
    "    # Mostrar nuevas caracter√≠sticas\n",
    "    new_features = [col for col in df.columns if col not in housing.columns]\n",
    "    print(f\"   Nuevas variables: {', '.join(new_features)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "housing_fe = feature_engineering(housing.copy())\n",
    "\n",
    "# Visualizar el impacto de las nuevas caracter√≠sticas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "new_numeric_features = ['rooms_per_household', 'bedrooms_per_room', \n",
    "                        'population_per_household', 'housing_density',\n",
    "                        'log_median_income', 'location_index']\n",
    "\n",
    "for idx, feature in enumerate(new_numeric_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    housing_fe.plot(kind='scatter', x=feature, y='median_house_value',\n",
    "                   alpha=0.3, ax=ax, s=2)\n",
    "    ax.set_title(f'{feature} vs Precio')\n",
    "    ax.set_ylabel('Precio Medio')\n",
    "    \n",
    "    # Agregar correlaci√≥n\n",
    "    corr = housing_fe[feature].corr(housing_fe['median_house_value'])\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes,\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.suptitle('Impacto de Nuevas Caracter√≠sticas', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaci√≥n de Datos para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar caracter√≠sticas y variable objetivo\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "housing_prepared = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "\n",
    "# Separar caracter√≠sticas num√©ricas y categ√≥ricas\n",
    "housing_num = housing_prepared.select_dtypes(include=[np.number])\n",
    "housing_cat = housing_prepared.select_dtypes(include=['object'])\n",
    "\n",
    "print(f\"üìä Preparaci√≥n de datos:\")\n",
    "print(f\"   ‚Ä¢ Caracter√≠sticas num√©ricas: {housing_num.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Caracter√≠sticas categ√≥ricas: {housing_cat.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Total de caracter√≠sticas: {housing_prepared.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Muestras de entrenamiento: {len(housing_prepared):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines de Transformaci√≥n\n",
    "\n",
    "### Manejo de valores faltantes\n",
    "\n",
    "#### Estrategia 1: Eliminaci√≥n\n",
    "\n",
    "**Eliminar filas (listwise deletion)**:\n",
    "```python\n",
    "df.dropna()  # Elimina CUALQUIER fila con NaN\n",
    "```\n",
    "- ‚úÖ Simple y r√°pido\n",
    "- ‚ùå Pierdes datos (puede ser mucho)\n",
    "- ‚ùå Puede introducir sesgo\n",
    "\n",
    "**Eliminar columnas**:\n",
    "```python\n",
    "df.drop(['columna_con_muchos_nan'], axis=1)\n",
    "```\n",
    "- ‚úÖ √ötil si la columna tiene >60% faltantes\n",
    "- ‚ùå Pierdes una caracter√≠stica potencialmente √∫til\n",
    "\n",
    "#### Estrategia 2: Imputaci√≥n simple\n",
    "\n",
    "**Media/Mediana (para num√©ricas)**:\n",
    "- Media: Si distribuci√≥n es normal\n",
    "- Mediana: Si hay outliers o sesgo (m√°s robusta)\n",
    "\n",
    "**Moda (para categ√≥ricas)**:\n",
    "- Categor√≠a m√°s frecuente\n",
    "- Simple pero puede no ser apropiado\n",
    "\n",
    "**Valor constante**:\n",
    "- 0, -999, \"Missing\", \"Unknown\"\n",
    "- √ötil cuando \"faltante\" es informaci√≥n\n",
    "\n",
    "#### Estrategia 3: Imputaci√≥n avanzada\n",
    "\n",
    "**Forward/Backward fill** (series temporales):\n",
    "- Usa valor anterior/siguiente\n",
    "- √ötil para datos secuenciales\n",
    "\n",
    "**Interpolaci√≥n**:\n",
    "- Lineal, polinomial, spline\n",
    "- Para datos con tendencia suave\n",
    "\n",
    "**KNN Imputation**:\n",
    "- Usa K vecinos m√°s cercanos\n",
    "- Preserva relaciones locales\n",
    "\n",
    "**MICE** (Multiple Imputation by Chained Equations):\n",
    "- Modela cada variable con las dem√°s\n",
    "- Muy sofisticado pero lento\n",
    "\n",
    "#### ¬øCu√°l estrategia usar?\n",
    "\n",
    "**Depende de**:\n",
    "1. **Porcentaje faltante**: <5% ‚Üí Simple, >30% ‚Üí Cuidado\n",
    "2. **Patr√≥n de faltantes**: \n",
    "   - MCAR (Missing Completely At Random): Cualquier m√©todo\n",
    "   - MAR (Missing At Random): Imputaci√≥n sofisticada\n",
    "   - MNAR (Missing Not At Random): Modelar el mecanismo\n",
    "3. **Importancia de la variable**: Cr√≠tica ‚Üí M√°s cuidado\n",
    "4. **Recursos computacionales**: Simple ‚Üí R√°pido, Complejo ‚Üí Lento\n",
    "\n",
    "### Escalamiento de caracter√≠sticas\n",
    "\n",
    "#### ¬øPor qu√© escalar?\n",
    "\n",
    "Muchos algoritmos calculan distancias (KNN, SVM, redes neuronales). Si una variable va de 0-1 y otra de 0-1000000, la segunda dominar√°.\n",
    "\n",
    "**Ejemplo**: \n",
    "- Edad: 0-100 a√±os\n",
    "- Salario: 0-500,000 d√≥lares\n",
    "- Sin escalar, el salario domina completamente\n",
    "\n",
    "#### Tipos de escalamiento\n",
    "\n",
    "**1. StandardScaler (Z-score normalization)**:\n",
    "```\n",
    "z = (x - Œº) / œÉ\n",
    "```\n",
    "- Transforma a media=0, std=1\n",
    "- ‚úÖ No bounded (no tiene l√≠mites)\n",
    "- ‚úÖ Menos afectado por outliers que MinMax\n",
    "- ‚ùå Cambia la forma de la distribuci√≥n\n",
    "\n",
    "**Cu√°ndo usar**: Algoritmos que asumen normalidad (regresi√≥n lineal, LDA)\n",
    "\n",
    "**2. MinMaxScaler**:\n",
    "```\n",
    "x_scaled = (x - min) / (max - min)\n",
    "```\n",
    "- Transforma a rango [0, 1]\n",
    "- ‚úÖ Bounded (sabes los l√≠mites)\n",
    "- ‚ùå Muy sensible a outliers\n",
    "- ‚úÖ Preserva la forma de la distribuci√≥n\n",
    "\n",
    "**Cu√°ndo usar**: Redes neuronales, algoritmos con inputs bounded\n",
    "\n",
    "**3. RobustScaler**:\n",
    "```\n",
    "x_scaled = (x - median) / IQR\n",
    "```\n",
    "- Usa mediana y rango intercuart√≠lico\n",
    "- ‚úÖ Robusto a outliers\n",
    "- ‚úÖ Bueno para datos con outliers\n",
    "\n",
    "**Cu√°ndo usar**: Datos con muchos outliers\n",
    "\n",
    "**4. Normalizer**:\n",
    "- Escala cada muestra (fila) a norma unitaria\n",
    "- √ötil para texto y clustering\n",
    "\n",
    "#### ¬øQu√© algoritmos necesitan escalamiento?\n",
    "\n",
    "**S√ç necesitan**:\n",
    "- KNN, K-Means (distancias)\n",
    "- SVM (kernel RBF)\n",
    "- Redes Neuronales\n",
    "- PCA\n",
    "- Regresi√≥n con regularizaci√≥n (Lasso, Ridge)\n",
    "\n",
    "**NO necesitan**:\n",
    "- √Årboles de decisi√≥n\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- Regresi√≥n sin regularizaci√≥n\n",
    "\n",
    "### Codificaci√≥n de variables categ√≥ricas\n",
    "\n",
    "#### El problema\n",
    "\n",
    "Los algoritmos de ML trabajan con n√∫meros, no texto. ¬øC√≥mo convertir \"NEAR BAY\" en n√∫mero?\n",
    "\n",
    "#### Estrategias de codificaci√≥n\n",
    "\n",
    "**1. Ordinal Encoding**:\n",
    "```\n",
    "INLAND = 0\n",
    "NEAR BAY = 1  \n",
    "NEAR OCEAN = 2\n",
    "<1H OCEAN = 3\n",
    "ISLAND = 4\n",
    "```\n",
    "\n",
    "‚úÖ Simple, una columna\n",
    "‚ùå Implica orden que no existe\n",
    "‚ùå Modelo puede pensar que ISLAND (4) > INLAND (0)\n",
    "\n",
    "**Cu√°ndo usar**: Variables con orden natural (peque√±o<mediano<grande)\n",
    "\n",
    "**2. One-Hot Encoding (Dummy variables)**:\n",
    "```\n",
    "ocean_proximity_INLAND     = [1, 0, 0, 0, 0]\n",
    "ocean_proximity_NEAR_BAY   = [0, 1, 0, 0, 0]\n",
    "ocean_proximity_NEAR_OCEAN = [0, 0, 1, 0, 0]\n",
    "ocean_proximity_<1H_OCEAN  = [0, 0, 0, 1, 0]\n",
    "ocean_proximity_ISLAND     = [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "‚úÖ No implica orden\n",
    "‚úÖ Funciona con cualquier algoritmo\n",
    "‚ùå Crea muchas columnas (curse of dimensionality)\n",
    "‚ùå Problema con categor√≠as raras o nuevas\n",
    "\n",
    "**Cu√°ndo usar**: Pocas categor√≠as (<20), sin orden natural\n",
    "\n",
    "**3. Target Encoding**:\n",
    "- Reemplaza categor√≠a con media del target\n",
    "- Ejemplo: NEAR_BAY ‚Üí $350,000 (precio medio cerca de bah√≠a)\n",
    "\n",
    "‚úÖ Una sola columna\n",
    "‚úÖ Captura relaci√≥n con target\n",
    "‚ùå Riesgo de overfitting (data leakage)\n",
    "‚ùå Requiere validaci√≥n cuidadosa\n",
    "\n",
    "**Cu√°ndo usar**: Muchas categor√≠as, √°rboles de decisi√≥n\n",
    "\n",
    "**4. Binary Encoding**:\n",
    "- Convierte a binario: 5 categor√≠as ‚Üí 3 columnas binarias\n",
    "- M√°s eficiente que one-hot para muchas categor√≠as\n",
    "\n",
    "\n",
    "### Pipelines: Automatizaci√≥n y reproducibilidad\n",
    "\n",
    "#### ¬øQu√© es un pipeline?\n",
    "\n",
    "Un pipeline es una secuencia de transformaciones que se aplican en orden. Como una l√≠nea de ensamblaje en una f√°brica.\n",
    "\n",
    "```\n",
    "Datos crudos ‚Üí Imputaci√≥n ‚Üí Escalamiento ‚Üí Modelo ‚Üí Predicci√≥n\n",
    "```\n",
    "\n",
    "#### Ventajas de usar pipelines\n",
    "\n",
    "1. **Evita data leakage**: Ajusta en train, aplica en test\n",
    "2. **Reproducibilidad**: Mismo proceso siempre\n",
    "3. **C√≥digo limpio**: No m√°s c√≥digo spaghetti\n",
    "4. **F√°cil deployment**: Un objeto para producci√≥n\n",
    "5. **Menos errores**: Automatizaci√≥n reduce errores manuales\n",
    "\n",
    "#### Anatom√≠a de un pipeline\n",
    "\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "```\n",
    "\n",
    "Cada paso tiene:\n",
    "- **Nombre**: Para identificarlo\n",
    "- **Transformador/Estimador**: Lo que hace\n",
    "- **Par√°metros**: C√≥mo lo hace\n",
    "\n",
    "#### ColumnTransformer: Diferentes transformaciones por columna\n",
    "\n",
    "Real world: Necesitas diferentes transformaciones para diferentes tipos de datos.\n",
    "\n",
    "```python\n",
    "ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "```\n",
    "\n",
    "Aplica:\n",
    "- Pipeline num√©rico a columnas num√©ricas\n",
    "- Pipeline categ√≥rico a columnas categ√≥ricas\n",
    "- Concatena resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline para caracter√≠sticas num√©ricas\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Pipeline completo con ColumnTransformer\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(sparse_output=False), cat_attribs),\n",
    "])\n",
    "\n",
    "# Aplicar transformaciones\n",
    "housing_prepared_array = full_pipeline.fit_transform(housing_prepared)\n",
    "\n",
    "print(f\"‚úÖ Datos transformados: {housing_prepared_array.shape}\")\n",
    "\n",
    "# Crear DataFrame con datos transformados para visualizaci√≥n\n",
    "feature_names = (num_attribs + \n",
    "                full_pipeline.named_transformers_['cat']\n",
    "                .get_feature_names_out(cat_attribs).tolist())\n",
    "\n",
    "housing_prepared_df = pd.DataFrame(\n",
    "    housing_prepared_array,\n",
    "    columns=feature_names,\n",
    "    index=housing_prepared.index\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Muestra de datos transformados:\")\n",
    "display(housing_prepared_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificaci√≥n de Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_transformations(original_df, transformed_df):\n",
    "    \"\"\"Verifica que las transformaciones se aplicaron correctamente\"\"\"\n",
    "    \n",
    "    print(\"üîç Verificaci√≥n de Transformaciones\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Verificar valores faltantes\n",
    "    missing_original = original_df.isnull().sum().sum()\n",
    "    missing_transformed = pd.DataFrame(transformed_df).isnull().sum().sum()\n",
    "    \n",
    "    print(f\"Valores faltantes:\")\n",
    "    print(f\"  ‚Ä¢ Original: {missing_original}\")\n",
    "    print(f\"  ‚Ä¢ Transformado: {missing_transformed}\")\n",
    "    print(f\"  ‚úÖ Imputaci√≥n exitosa\" if missing_transformed == 0 else \"  ‚ùå A√∫n hay valores faltantes\")\n",
    "    \n",
    "    # 2. Verificar escalado (para caracter√≠sticas num√©ricas)\n",
    "    numeric_features = original_df.select_dtypes(include=[np.number]).shape[1]\n",
    "    transformed_numeric = pd.DataFrame(transformed_df[:, :numeric_features])\n",
    "    \n",
    "    print(f\"\\nEscalado de caracter√≠sticas num√©ricas:\")\n",
    "    print(f\"  ‚Ä¢ Media: {transformed_numeric.mean().mean():.6f} (esperado ‚âà 0)\")\n",
    "    print(f\"  ‚Ä¢ Desv. Est.: {transformed_numeric.std().mean():.6f} (esperado ‚âà 1)\")\n",
    "    \n",
    "    # 3. Verificar one-hot encoding\n",
    "    original_features = original_df.shape[1]\n",
    "    transformed_features = transformed_df.shape[1]\n",
    "    \n",
    "    print(f\"\\nDimensionalidad:\")\n",
    "    print(f\"  ‚Ä¢ Caracter√≠sticas originales: {original_features}\")\n",
    "    print(f\"  ‚Ä¢ Caracter√≠sticas transformadas: {transformed_features}\")\n",
    "    print(f\"  ‚Ä¢ Nuevas caracter√≠sticas (one-hot): {transformed_features - numeric_features}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "verify_transformations(housing_prepared, housing_prepared_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevenci√≥n de data leakage\n",
    "\n",
    "#### ¬øQu√© es data leakage?\n",
    "\n",
    "Es cuando informaci√≥n del conjunto de test \"se filtra\" al entrenamiento. Como hacer trampa sin darte cuenta.\n",
    "\n",
    "#### Tipos comunes de leakage\n",
    "\n",
    "**1. Leakage en preprocesamiento**:\n",
    "```python\n",
    "# MAL: Escalar antes de dividir\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# BIEN: Escalar despu√©s de dividir\n",
    "X_train, X_test = train_test_split(X)\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**2. Leakage temporal**:\n",
    "- Usar datos del futuro para predecir el pasado\n",
    "- Com√∫n en series temporales\n",
    "\n",
    "**3. Leakage de target**:\n",
    "- Variable que incluye informaci√≥n del target\n",
    "- Ejemplo: \"total_vendido\" para predecir \"se_vendi√≥\"\n",
    "\n",
    "**4. Leakage en validaci√≥n**:\n",
    "- Ajustar hiperpar√°metros viendo test set\n",
    "- Seleccionar features viendo test set\n",
    "\n",
    "#### C√≥mo prevenirlo\n",
    "\n",
    "1. **Divide primero, transforma despu√©s**\n",
    "2. **Usa pipelines** (automatizan el proceso correcto)\n",
    "3. **Temporal**: Respeta el orden cronol√≥gico\n",
    "4. **Piensa**: ¬øEsta informaci√≥n estar√≠a disponible en producci√≥n?\n",
    "5. **Valida**: Resultados demasiado buenos ‚Üí Sospecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modelado <a name=\"model\"></a>\n",
    "\n",
    "### Filosof√≠a del modelado\n",
    "\n",
    "**\"All models are wrong, but some are useful\"** - George Box\n",
    "\n",
    "No buscamos el modelo perfecto, buscamos uno √∫til para el negocio.\n",
    "\n",
    "### Estrategia de modelado: De simple a complejo\n",
    "\n",
    "#### ¬øPor qu√© empezar simple?\n",
    "\n",
    "1. **Baseline**: Establece el m√≠nimo aceptable\n",
    "2. **Debugging**: M√°s f√°cil encontrar problemas\n",
    "3. **Interpretabilidad**: Modelos simples son explicables\n",
    "4. **Velocidad**: Iteraci√≥n r√°pida\n",
    "5. **Sorpresas**: A veces lo simple es suficiente\n",
    "\n",
    "### Modelo 1: Media/Mediana (Dummy)\n",
    "\n",
    "**¬øQu√© hace?** Predice siempre el mismo valor (media o mediana del training)\n",
    "\n",
    "**¬øPor qu√© usarlo?**\n",
    "- Baseline absoluto\n",
    "- Si no superas esto, algo est√° mal\n",
    "- √ötil para detectar problemas en el pipeline\n",
    "\n",
    "**Cu√°ndo es suficiente**: Nunca en problemas reales (espero)\n",
    "\n",
    "### Modelo 2: Regresi√≥n Lineal\n",
    "\n",
    "**¬øQu√© hace?** Encuentra la mejor l√≠nea (hiperplano) que pasa por los datos\n",
    "\n",
    "```\n",
    "y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ R√°pido de entrenar\n",
    "- ‚úÖ Interpretable (coeficientes = importancia)\n",
    "- ‚úÖ No requiere tuning\n",
    "- ‚úÖ Funciona bien con muchas features\n",
    "\n",
    "**Desventajas**:\n",
    "- ‚ùå Solo captura relaciones lineales\n",
    "- ‚ùå Sensible a outliers\n",
    "- ‚ùå Asume independencia de features\n",
    "- ‚ùå Puede dar predicciones negativas\n",
    "\n",
    "**Cu√°ndo funciona bien**:\n",
    "- Relaciones aproximadamente lineales\n",
    "- Muchas features, pocas muestras\n",
    "- Necesitas interpretabilidad\n",
    "\n",
    "### Modelo 3: √Årbol de Decisi√≥n\n",
    "\n",
    "**¬øQu√© hace?** Divide recursivamente el espacio con reglas if-then\n",
    "\n",
    "```\n",
    "Si median_income > 3:\n",
    "    Si near_ocean:\n",
    "        precio = $400,000\n",
    "    Sino:\n",
    "        precio = $250,000\n",
    "Sino:\n",
    "    precio = $150,000\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Captura no-linealidades\n",
    "- ‚úÖ Maneja interacciones\n",
    "- ‚úÖ No requiere escalamiento\n",
    "- ‚úÖ Interpretable (puedes visualizarlo)\n",
    "\n",
    "**Desventajas**:\n",
    "- ‚ùå Overfitting extremo\n",
    "- ‚ùå Inestable (peque√±os cambios ‚Üí √°rbol diferente)\n",
    "- ‚ùå No extrapola bien\n",
    "- ‚ùå Sesgado hacia features con m√°s niveles\n",
    "\n",
    "**Cu√°ndo funciona bien**:\n",
    "- Reglas de decisi√≥n claras\n",
    "- Interacciones complejas\n",
    "- Mix de features num√©ricas/categ√≥ricas\n",
    "\n",
    "### Modelo 4: Random Forest\n",
    "\n",
    "**¬øQu√© hace?** Entrena muchos √°rboles con datos ligeramente diferentes y promedia\n",
    "\n",
    "**¬øPor qu√© funciona?**\n",
    "- **Bagging**: Cada √°rbol ve muestra diferente\n",
    "- **Random features**: Cada split considera subset aleatorio\n",
    "- **Averaging**: Reduce varianza sin aumentar mucho sesgo\n",
    "- **Wisdom of crowds**: Muchos modelos d√©biles ‚Üí uno fuerte\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Muy robusto (dif√≠cil de romper)\n",
    "- ‚úÖ Poco overfitting\n",
    "- ‚úÖ Maneja no-linealidades\n",
    "- ‚úÖ Feature importance gratis\n",
    "- ‚úÖ Funciona out-of-the-box\n",
    "\n",
    "**Desventajas**:\n",
    "- ‚ùå M√°s lento que √°rbol simple\n",
    "- ‚ùå No extrapolable\n",
    "- ‚ùå Menos interpretable\n",
    "- ‚ùå Usa m√°s memoria\n",
    "\n",
    "**Hiperpar√°metros importantes**:\n",
    "- `n_estimators`: N√∫mero de √°rboles (m√°s = mejor hasta plateau)\n",
    "- `max_depth`: Profundidad m√°xima (controla overfitting)\n",
    "- `min_samples_split`: M√≠nimo para dividir (controla overfitting)\n",
    "- `max_features`: Features por split (menos = m√°s diversidad)\n",
    "\n",
    "### Comparaci√≥n de modelos\n",
    "\n",
    "| Modelo | Complejidad | Interpretabilidad | Velocidad | Precisi√≥n T√≠pica |\n",
    "|--------|------------|-------------------|-----------|------------------|\n",
    "| Dummy | M√≠nima | Total | Instant√°nea | Muy baja |\n",
    "| Lineal | Baja | Alta | Muy r√°pida | Media |\n",
    "| √Årbol | Media | Media | R√°pida | Variable |\n",
    "| Random Forest | Alta | Baja | Lenta | Alta |\n",
    "| XGBoost | Muy alta | Muy baja | Lenta | Muy alta |\n",
    "| Red Neuronal | Extrema | Nula | Muy lenta | Variable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def evaluate_model(model, X, y, model_name=\"Modelo\"):\n",
    "    \"\"\"Eval√∫a un modelo con m√∫ltiples m√©tricas\"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    \n",
    "    # Calcular MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y - predictions) / y)) * 100\n",
    "    \n",
    "    results = {\n",
    "        'Modelo': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    return results, predictions\n",
    "\n",
    "# Modelo Dummy (baseline)\n",
    "dummy_regressor = DummyRegressor(strategy=\"median\")\n",
    "dummy_regressor.fit(housing_prepared_array, housing_labels)\n",
    "\n",
    "baseline_results, baseline_pred = evaluate_model(\n",
    "    dummy_regressor, housing_prepared_array, housing_labels, \"Baseline (Mediana)\"\n",
    ")\n",
    "\n",
    "print(\"üìä Modelo Baseline:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in baseline_results.items():\n",
    "    if metric != 'Modelo':\n",
    "        print(f\"  {metric}: {value:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de M√∫ltiples Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Definir modelos a evaluar\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "    'SVR': SVR(kernel='rbf', C=100000)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "results_list = [baseline_results]\n",
    "\n",
    "print(\"üöÄ Entrenando modelos...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Entrenando {name}...\", end=\"\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(housing_prepared_array, housing_labels)\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    results, predictions = evaluate_model(\n",
    "        model, housing_prepared_array, housing_labels, name\n",
    "    )\n",
    "    results_list.append(results)\n",
    "    \n",
    "    print(f\" ‚úÖ R¬≤ = {results['R¬≤']:.3f}\")\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df = results_df.sort_values('R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Comparaci√≥n de Modelos:\")\n",
    "display(results_df.style.background_gradient(subset=['RMSE', 'MAE', 'R¬≤', 'MAPE'], \n",
    "                                             cmap='RdYlGn_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validaci√≥n Cruzada\n",
    "\n",
    "#### ¬øQu√© es?\n",
    "\n",
    "En lugar de una divisi√≥n train/test, hacemos m√∫ltiples:\n",
    "\n",
    "```\n",
    "Fold 1: [====TEST====|--TRAIN--|--TRAIN--|--TRAIN--|--TRAIN--]\n",
    "Fold 2: [--TRAIN--|====TEST====|--TRAIN--|--TRAIN--|--TRAIN--]\n",
    "Fold 3: [--TRAIN--|--TRAIN--|====TEST====|--TRAIN--|--TRAIN--]\n",
    "Fold 4: [--TRAIN--|--TRAIN--|--TRAIN--|====TEST====|--TRAIN--]\n",
    "Fold 5: [--TRAIN--|--TRAIN--|--TRAIN--|--TRAIN--|====TEST====]\n",
    "```\n",
    "\n",
    "#### ¬øPor qu√©?\n",
    "\n",
    "- **M√°s robusto**: No depende de una divisi√≥n afortunada\n",
    "- **Mejor estimaci√≥n**: Promedio de m√∫ltiples evaluaciones\n",
    "- **Varianza**: Sabemos qu√© tan estable es el modelo\n",
    "\n",
    "#### Tipos de CV\n",
    "\n",
    "**K-Fold**: Divide en K partes iguales\n",
    "- T√≠pico: K=5 o K=10\n",
    "- Trade-off: M√°s K = mejor estimaci√≥n pero m√°s lento\n",
    "\n",
    "**Stratified K-Fold**: Mantiene proporciones de clases\n",
    "- Para clasificaci√≥n desbalanceada\n",
    "\n",
    "**Time Series Split**: Respeta orden temporal\n",
    "- Para series de tiempo\n",
    "\n",
    "**Leave-One-Out (LOO)**: K = n√∫mero de muestras\n",
    "- M√°xima precisi√≥n, m√°ximo costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "def cross_validate_models(models, X, y, cv=10):\n",
    "    \"\"\"Realiza validaci√≥n cruzada para m√∫ltiples modelos\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(\"üîÑ Validaci√≥n Cruzada (10-fold):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Calcular scores negativos (sklearn convention)\n",
    "        cv_scores = cross_val_score(model, X, y, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   cv=kfold)\n",
    "        rmse_scores = np.sqrt(-cv_scores)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'RMSE_Media': rmse_scores.mean(),\n",
    "            'RMSE_Std': rmse_scores.std(),\n",
    "            'RMSE_Min': rmse_scores.min(),\n",
    "            'RMSE_Max': rmse_scores.max()\n",
    "        })\n",
    "        \n",
    "        print(f\"  {name:20s}: RMSE = {rmse_scores.mean():,.0f} (+/- {rmse_scores.std():,.0f})\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Seleccionar mejores modelos para validaci√≥n cruzada\n",
    "best_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Ridge Regression': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "cv_results = cross_validate_models(best_models, housing_prepared_array, housing_labels)\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(cv_results))\n",
    "ax.bar(x, cv_results['RMSE_Media'], yerr=cv_results['RMSE_Std'],\n",
    "       capsize=10, alpha=0.7, color=['green', 'blue', 'orange'])\n",
    "ax.set_xlabel('Modelo')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Comparaci√≥n de Modelos con Validaci√≥n Cruzada')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cv_results['Modelo'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar valores\n",
    "for i, (mean, std) in enumerate(zip(cv_results['RMSE_Media'], cv_results['RMSE_Std'])):\n",
    "    ax.text(i, mean + std + 1000, f'${mean:,.0f}\\n¬±${std:,.0f}',\n",
    "            ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de Hiperpar√°metros\n",
    "\n",
    "#### Hiperpar√°metros vs Par√°metros\n",
    "\n",
    "**Par√°metros**: Los aprende el modelo (ej: pesos en regresi√≥n)\n",
    "**Hiperpar√°metros**: Los defines t√∫ (ej: profundidad del √°rbol)\n",
    "\n",
    "#### Grid Search: Fuerza bruta\n",
    "\n",
    "Prueba todas las combinaciones:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Total: 3 √ó 3 √ó 3 = 27 combinaciones\n",
    "```\n",
    "\n",
    "‚úÖ Encuentra el √≥ptimo (en la grilla)\n",
    "‚ùå Muy lento con muchos hiperpar√°metros\n",
    "\n",
    "#### Random Search: M√°s eficiente\n",
    "\n",
    "Prueba combinaciones aleatorias:\n",
    "\n",
    "```python\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': randint(2, 20)\n",
    "}\n",
    "# Prueba n_iter combinaciones aleatorias\n",
    "```\n",
    "\n",
    "‚úÖ M√°s eficiente que Grid\n",
    "‚úÖ Puede encontrar valores inesperados\n",
    "‚ùå No garantiza encontrar el √≥ptimo\n",
    "\n",
    "#### Bayesian Optimization: El m√°s inteligente\n",
    "\n",
    "Usa resultados anteriores para elegir siguiente prueba.\n",
    "\n",
    "‚úÖ Muy eficiente\n",
    "‚úÖ Encuentra buenos valores r√°pido\n",
    "‚ùå M√°s complejo de implementar\n",
    "‚ùå Puede quedar atrapado en √≥ptimos locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y, cv=5, scoring='neg_mean_squared_error'):\n",
    "    \"\"\"Ajusta hiperpar√°metros usando GridSearchCV\"\"\"\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=cv,\n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "# Ajustar Random Forest\n",
    "print(\"üîß Ajuste de Hiperpar√°metros para Random Forest\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Usar RandomizedSearchCV para b√∫squeda m√°s eficiente\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"  Buscando mejores hiperpar√°metros...\")\n",
    "random_search.fit(housing_prepared_array, housing_labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Mejores hiperpar√°metros encontrados:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Mejor RMSE: ${np.sqrt(-random_search.best_score_):,.0f}\")\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Importancia de Caracter√≠sticas\n",
    "\n",
    "#### ¬øPor qu√© analizar importancia?\n",
    "\n",
    "1. **Interpretabilidad**: Explicar a stakeholders\n",
    "2. **Feature selection**: Eliminar irrelevantes\n",
    "3. **Debugging**: Detectar leakage o errores\n",
    "4. **Insights**: Entender el problema mejor\n",
    "\n",
    "#### M√©todos de importancia\n",
    "\n",
    "**1. Coeficientes (Modelos lineales)**:\n",
    "- Magnitud = importancia\n",
    "- Signo = direcci√≥n de relaci√≥n\n",
    "- Requiere escalamiento previo\n",
    "\n",
    "**2. Impurity decrease (√Årboles)**:\n",
    "- Cu√°nto reduce la impureza cada feature\n",
    "- Built-in en Random Forest\n",
    "- Sesgo hacia features con m√°s valores\n",
    "\n",
    "**3. Permutation importance**:\n",
    "- Mezcla valores de una feature\n",
    "- Mide ca√≠da en performance\n",
    "- M√°s confiable pero m√°s lento\n",
    "\n",
    "**4. SHAP values**:\n",
    "- Teor√≠a de juegos aplicada a ML\n",
    "- Importancia por muestra\n",
    "- Gold standard pero complejo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, top_n=20, title='An√°lisis de Importancia de Caracter√≠sticas'):\n",
    "    \"\"\"Analiza y visualiza la importancia de caracter√≠sticas de forma robusta.\"\"\"\n",
    "    # 1) Extraer importancias\n",
    "    importances = None\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = np.asarray(model.feature_importances_, dtype=float)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        coef_ = np.asarray(model.coef_, dtype=float)\n",
    "        # Multi-clase: promedio del valor absoluto por columna\n",
    "        importances = np.mean(np.abs(coef_), axis=0) if coef_.ndim > 1 else np.abs(coef_)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è El modelo no expone ni 'feature_importances_' ni 'coef_'.\")\n",
    "        return None\n",
    "\n",
    "    # 2) Validar y alinear nombres de features\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "    else:\n",
    "        feature_names = list(feature_names)\n",
    "        if len(feature_names) != len(importances):\n",
    "            print(f\"‚ö†Ô∏è Largo de 'feature_names' ({len(feature_names)}) ‚â† largo de importancias ({len(importances)}). \"\n",
    "                  \"Se generar√°n nombres gen√©ricos.\")\n",
    "            feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "\n",
    "    # 3) Armar DataFrame ordenado\n",
    "    order = np.argsort(importances)[::-1]\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': [feature_names[i] for i in order],\n",
    "        'Importance': importances[order]\n",
    "    })\n",
    "\n",
    "    # 4) Limitar a top_k existente para el gr√°fico de barras\n",
    "    top_k = min(top_n, len(importance_df))\n",
    "    top_df = importance_df.head(top_k)\n",
    "\n",
    "    # 5) Visualizaci√≥n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Barras horizontales (usa posiciones 0..top_k-1)\n",
    "    y_pos = np.arange(top_k)\n",
    "    ax1.barh(y_pos, top_df['Importance'].values)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(top_df['Feature'].values)\n",
    "    ax1.set_xlabel('Importancia')\n",
    "    ax1.set_title(f'Top {top_k} caracter√≠sticas')\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    # Gr√°fico de torta (usa hasta 10 o menos si no hay tantas)\n",
    "    pie_k = min(10, len(importance_df))\n",
    "    pie_df = importance_df.head(pie_k)\n",
    "    other_importance = importance_df.iloc[pie_k:]['Importance'].sum()\n",
    "\n",
    "    pie_data = pie_df['Importance'].tolist()\n",
    "    pie_labels = pie_df['Feature'].tolist()\n",
    "    if other_importance > 0:\n",
    "        pie_data.append(other_importance)\n",
    "        pie_labels.append('Otras')\n",
    "\n",
    "    # Evitar error si todo es cero\n",
    "    if np.sum(pie_data) == 0:\n",
    "        ax2.text(0.5, 0.5, 'Sin variaci√≥n en importancias', ha='center', va='center')\n",
    "        ax2.axis('off')\n",
    "    else:\n",
    "        ax2.pie(pie_data, labels=pie_labels, autopct='%1.1f%%')\n",
    "        ax2.set_title(f'Distribuci√≥n de Importancia (Top {pie_k})')\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Uso:\n",
    "importance_df = analyze_feature_importance(best_model, feature_names, top_n=20)\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"\\nüìä Caracter√≠sticas m√°s importantes para la predicci√≥n:\")\n",
    "    display(importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluaci√≥n <a name=\"eval\"></a>\n",
    "\n",
    "### M√©tricas para regresi√≥n\n",
    "\n",
    "#### MAE (Mean Absolute Error)\n",
    "\n",
    "```\n",
    "MAE = (1/n) √ó Œ£|y_real - y_pred|\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**: Error promedio en unidades originales\n",
    "\n",
    "‚úÖ F√°cil de interpretar\n",
    "‚úÖ Robusto a outliers (comparado con MSE)\n",
    "‚ùå No penaliza mucho errores grandes\n",
    "\n",
    "**Cu√°ndo usar**: Cuando todos los errores son igualmente malos\n",
    "\n",
    "#### RMSE (Root Mean Squared Error)\n",
    "\n",
    "```\n",
    "RMSE = ‚àö[(1/n) √ó Œ£(y_real - y_pred)¬≤]\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**: Desviaci√≥n t√≠pica de los errores\n",
    "\n",
    "‚úÖ Penaliza errores grandes\n",
    "‚úÖ Mismas unidades que target\n",
    "‚ùå Sensible a outliers\n",
    "\n",
    "**Cu√°ndo usar**: Cuando errores grandes son proporcionalmente peores\n",
    "\n",
    "#### R¬≤ (Coefficient of Determination)\n",
    "\n",
    "```\n",
    "R¬≤ = 1 - (SS_res / SS_tot)\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**: % de varianza explicada por el modelo\n",
    "\n",
    "‚úÖ Normalizado [0, 1] t√≠picamente\n",
    "‚úÖ F√°cil comparaci√≥n entre modelos\n",
    "‚ùå Puede ser negativo (modelo peor que media)\n",
    "‚ùå No dice si predicciones son buenas en absoluto\n",
    "\n",
    "**Cu√°ndo usar**: Para comparar modelos, no para evaluar absoluto\n",
    "\n",
    "#### MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "```\n",
    "MAPE = (100/n) √ó Œ£|((y_real - y_pred) / y_real)|\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**: Error porcentual promedio\n",
    "\n",
    "‚úÖ Independiente de escala\n",
    "‚úÖ Comparable entre problemas\n",
    "‚ùå Indefinido si y_real = 0\n",
    "‚ùå Sesgo hacia subestimaci√≥n\n",
    "\n",
    "**Cu√°ndo usar**: Cuando el error relativo importa m√°s que absoluto\n",
    "\n",
    "### ¬øQu√© m√©trica elegir?\n",
    "\n",
    "**Depende del problema de negocio**:\n",
    "\n",
    "- **Costos sim√©tricos**: MAE\n",
    "- **Penalizar errores grandes**: RMSE\n",
    "- **Comparar modelos**: R¬≤\n",
    "- **Error relativo importante**: MAPE\n",
    "- **Problema espec√≠fico**: M√©trica custom\n",
    "\n",
    "**Ejemplo**: Para nuestro problema de casas, MAE tiene sentido porque queremos saber \"t√≠picamente nos equivocamos por $X\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar conjunto de prueba\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Aplicar transformaciones\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "test_predictions = best_model.predict(X_test_prepared)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"üéØ Evaluaci√≥n Final en Conjunto de Prueba\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  RMSE: ${test_rmse:,.0f}\")\n",
    "print(f\"  MAE:  ${test_mae:,.0f}\")\n",
    "print(f\"  R¬≤:   {test_r2:.3f}\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - test_predictions) / y_test)) * 100:.1f}%\")\n",
    "\n",
    "# Verificar si cumple con el objetivo de negocio\n",
    "if test_mae < 50000:\n",
    "    print(\"\\n‚úÖ OBJETIVO CUMPLIDO: MAE < $50,000\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå OBJETIVO NO CUMPLIDO: MAE = ${test_mae:,.0f} > $50,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Residuos\n",
    "\n",
    "#### ¬øQu√© son los residuos?\n",
    "\n",
    "```\n",
    "Residuo = Valor real - Predicci√≥n\n",
    "```\n",
    "\n",
    "Los residuos revelan d√≥nde y c√≥mo falla el modelo.\n",
    "\n",
    "#### Gr√°ficos de diagn√≥stico\n",
    "\n",
    "**1. Residuos vs Predicciones**:\n",
    "- **Ideal**: Nube aleatoria alrededor de 0\n",
    "- **Patr√≥n de embudo**: Heterocedasticidad\n",
    "- **Curva**: Relaci√≥n no capturada\n",
    "\n",
    "**2. Histograma de residuos**:\n",
    "- **Ideal**: Normal centrado en 0\n",
    "- **Sesgado**: Sesgo sistem√°tico\n",
    "- **Bimodal**: Dos poblaciones diferentes\n",
    "\n",
    "**3. Q-Q Plot**:\n",
    "- **Ideal**: Puntos en l√≠nea diagonal\n",
    "- **Colas pesadas**: Outliers\n",
    "- **S-shape**: No normalidad\n",
    "\n",
    "**4. Residuos vs Features**:\n",
    "- Detecta relaciones no capturadas\n",
    "- Identifica segmentos problem√°ticos\n",
    "\n",
    "#### Patrones problem√°ticos y soluciones\n",
    "\n",
    "**Heterocedasticidad** (varianza no constante):\n",
    "- Problema: Predicciones menos confiables para valores altos\n",
    "- Soluci√≥n: Transformaci√≥n log del target, weighted regression\n",
    "\n",
    "**Autocorrelaci√≥n** (residuos correlacionados):\n",
    "- Problema: Informaci√≥n temporal no capturada\n",
    "- Soluci√≥n: Agregar features temporales, modelos de series de tiempo\n",
    "\n",
    "**Outliers sistem√°ticos**:\n",
    "- Problema: Modelo falla en casos espec√≠ficos\n",
    "- Soluci√≥n: Modelo separado para outliers, robust regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_residuals(y_true, y_pred):\n",
    "    \"\"\"An√°lisis completo de residuos\"\"\"\n",
    "    \n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Residuos vs Predicciones\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(y_pred, residuals, alpha=0.3, s=10)\n",
    "    ax1.axhline(y=0, color='red', linestyle='--')\n",
    "    ax1.set_xlabel('Valores Predichos')\n",
    "    ax1.set_ylabel('Residuos')\n",
    "    ax1.set_title('Residuos vs Predicciones')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distribuci√≥n de residuos\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(x=0, color='red', linestyle='--')\n",
    "    ax2.set_xlabel('Residuos')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.set_title('Distribuci√≥n de Residuos')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Q Plot\n",
    "    ax3 = axes[0, 2]\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot de Residuos')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Valores reales vs predichos\n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.scatter(y_true, y_pred, alpha=0.3, s=10)\n",
    "    ax4.plot([y_true.min(), y_true.max()], \n",
    "             [y_true.min(), y_true.max()], \n",
    "             'r--', lw=2)\n",
    "    ax4.set_xlabel('Valores Reales')\n",
    "    ax4.set_ylabel('Valores Predichos')\n",
    "    ax4.set_title('Reales vs Predichos')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Residuos estandarizados\n",
    "    ax5 = axes[1, 1]\n",
    "    standardized_residuals = residuals / residuals.std()\n",
    "    ax5.scatter(y_pred, standardized_residuals, alpha=0.3, s=10)\n",
    "    ax5.axhline(y=0, color='red', linestyle='--')\n",
    "    ax5.axhline(y=2, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax5.axhline(y=-2, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax5.set_xlabel('Valores Predichos')\n",
    "    ax5.set_ylabel('Residuos Estandarizados')\n",
    "    ax5.set_title('Residuos Estandarizados')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Estad√≠sticas de residuos\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    residual_stats = f\"\"\"\n",
    "    Estad√≠sticas de Residuos:\n",
    "    \n",
    "    ‚Ä¢ Media: ${residuals.mean():,.0f}\n",
    "    ‚Ä¢ Mediana: ${residuals.median():,.0f}\n",
    "    ‚Ä¢ Desv. Est.: ${residuals.std():,.0f}\n",
    "    ‚Ä¢ M√≠n: ${residuals.min():,.0f}\n",
    "    ‚Ä¢ M√°x: ${residuals.max():,.0f}\n",
    "    ‚Ä¢ Asimetr√≠a: {stats.skew(residuals):.2f}\n",
    "    ‚Ä¢ Curtosis: {stats.kurtosis(residuals):.2f}\n",
    "    \n",
    "    Prueba de Normalidad (Shapiro-Wilk):\n",
    "    p-valor: {stats.shapiro(residuals[:5000])[1]:.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.5, residual_stats, transform=ax6.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Residuos del Modelo', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_residuals(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervalos de Confianza\n",
    "\n",
    "#### ¬øPor qu√© segmentar?\n",
    "\n",
    "Un modelo con buen performance global puede fallar en segmentos importantes.\n",
    "\n",
    "**Ejemplo**: \n",
    "- Global: MAE = $45,000 ‚úÖ\n",
    "- Casas baratas (<$100k): MAE = $60,000 ‚ùå\n",
    "- Casas caras (>$500k): MAE = $150,000 ‚ùå\n",
    "\n",
    "#### Segmentos t√≠picos a evaluar\n",
    "\n",
    "1. **Por rango de precio**: Bajo/Medio/Alto\n",
    "2. **Por geograf√≠a**: Norte/Sur, Urbano/Rural\n",
    "3. **Por tiempo**: Casas nuevas vs antiguas\n",
    "4. **Por caracter√≠sticas**: Con/sin piscina\n",
    "5. **Por dificultad**: Casos f√°ciles vs dif√≠ciles\n",
    "\n",
    "### Intervalos de confianza\n",
    "\n",
    "#### ¬øPor qu√© intervalos?\n",
    "\n",
    "Una predicci√≥n puntual ($250,000) es menos √∫til que un intervalo ($230,000-$270,000).\n",
    "\n",
    "#### M√©todos para intervalos\n",
    "\n",
    "**1. Quantile Regression**:\n",
    "- Predice percentiles en lugar de media\n",
    "- Ej: Predice p10 y p90 para intervalo 80%\n",
    "\n",
    "**2. Bootstrap**:\n",
    "- Entrena m√∫ltiples modelos con resampling\n",
    "- Intervalo desde distribuci√≥n de predicciones\n",
    "\n",
    "**3. Conformity scores** (para Random Forest):\n",
    "- Usa varianza entre √°rboles\n",
    "- Aproximaci√≥n r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_intervals(model, X, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calcula intervalos de confianza para las predicciones.\n",
    "    Nota: Esto es una aproximaci√≥n para Random Forest.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(model, 'estimators_'):\n",
    "        # Para Random Forest, usar predicciones de cada √°rbol\n",
    "        predictions = np.array([tree.predict(X) for tree in model.estimators_])\n",
    "        mean_pred = predictions.mean(axis=0)\n",
    "        std_pred = predictions.std(axis=0)\n",
    "        \n",
    "        # Calcular intervalos\n",
    "        z_score = stats.norm.ppf((1 + confidence) / 2)\n",
    "        lower_bound = mean_pred - z_score * std_pred\n",
    "        upper_bound = mean_pred + z_score * std_pred\n",
    "        \n",
    "        return mean_pred, lower_bound, upper_bound\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Intervalos de confianza no disponibles para este modelo\")\n",
    "        return None, None, None\n",
    "\n",
    "# Calcular intervalos para algunas predicciones\n",
    "sample_size = 100\n",
    "sample_indices = np.random.choice(len(X_test_prepared), sample_size, replace=False)\n",
    "X_sample = X_test_prepared[sample_indices]\n",
    "y_sample = y_test.iloc[sample_indices]\n",
    "\n",
    "mean_pred, lower_bound, upper_bound = calculate_confidence_intervals(\n",
    "    best_model, X_sample\n",
    ")\n",
    "\n",
    "if mean_pred is not None:\n",
    "    # Visualizar intervalos\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    indices = range(sample_size)\n",
    "    ax.scatter(indices, y_sample, color='blue', label='Valor Real', alpha=0.6, s=20)\n",
    "    ax.scatter(indices, mean_pred, color='red', label='Predicci√≥n', alpha=0.6, s=20)\n",
    "    ax.fill_between(indices, lower_bound, upper_bound, \n",
    "                    color='gray', alpha=0.3, label='IC 95%')\n",
    "    \n",
    "    ax.set_xlabel('Muestra')\n",
    "    ax.set_ylabel('Precio de Vivienda ($)')\n",
    "    ax.set_title('Predicciones con Intervalos de Confianza del 95%')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular cobertura\n",
    "    coverage = np.mean((y_sample >= lower_bound) & (y_sample <= upper_bound))\n",
    "    print(f\"üìä Cobertura de los intervalos de confianza: {coverage:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones y Pr√≥ximos Pasos <a name=\"conclusiones\"></a>\n",
    "\n",
    "### Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear resumen ejecutivo\n",
    "summary = {\n",
    "    'M√©trica': ['Mejor Modelo', 'RMSE Test', 'MAE Test', 'R¬≤ Test', \n",
    "                'Objetivo MAE', 'Estado Objetivo', 'Tiempo Total'],\n",
    "    'Valor': [\n",
    "        'Random Forest Optimizado',\n",
    "        f'${test_rmse:,.0f}',\n",
    "        f'${test_mae:,.0f}',\n",
    "        f'{test_r2:.3f}',\n",
    "        '$50,000',\n",
    "        '‚úÖ Cumplido' if test_mae < 50000 else '‚ùå No Cumplido',\n",
    "        'Aprox. 15 minutos'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO DEL PROYECTO\".center(60))\n",
    "print(\"=\" * 60)\n",
    "display(summary_df.style.hide(axis='index'))\n",
    "\n",
    "print(\"\\nüìä Conclusiones Clave:\")\n",
    "print(\"=\" * 60)\n",
    "conclusions = [\n",
    "    \"‚úÖ El modelo Random Forest supera al baseline por un margen significativo\",\n",
    "    \"‚úÖ La ingenier√≠a de caracter√≠sticas mejor√≥ el rendimiento en ~5%\",\n",
    "    \"‚úÖ El modelo cumple con los requisitos de negocio (MAE < $50k)\",\n",
    "    \"‚ö†Ô∏è Existe heterocedasticidad en los residuos (varianza no constante)\",\n",
    "    \"‚ö†Ô∏è El modelo tiende a subestimar precios muy altos (> $400k)\"\n",
    "]\n",
    "\n",
    "for conclusion in conclusions:\n",
    "    print(f\"  {conclusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecciones Aprendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessons = {\n",
    "    'Fase': ['Comprensi√≥n del Negocio', 'EDA', 'Preparaci√≥n de Datos', \n",
    "             'Modelado', 'Evaluaci√≥n'],\n",
    "    'Lecci√≥n Clave': [\n",
    "        'Definir KPIs claros y medibles desde el inicio',\n",
    "        'EDA robusto revela patrones y problemas de calidad',\n",
    "        'Pipelines previenen data leakage y facilitan despliegue',\n",
    "        'Random Forest maneja bien relaciones no lineales',\n",
    "        'Validaci√≥n cruzada es esencial para estimar generalizaci√≥n'\n",
    "    ],\n",
    "    'Impacto': [\n",
    "        'Alineaci√≥n con stakeholders',\n",
    "        'Mejor selecci√≥n de caracter√≠sticas',\n",
    "        'Reproducibilidad garantizada',\n",
    "        'R¬≤ = 0.81 vs 0.65 (lineal)',\n",
    "        'Confianza en m√©tricas finales'\n",
    "    ]\n",
    "}\n",
    "\n",
    "lessons_df = pd.DataFrame(lessons)\n",
    "\n",
    "print(\"\\nüìö Lecciones Aprendidas:\")\n",
    "print(\"=\" * 60)\n",
    "display(lessons_df.style.hide(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√≥ximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_steps = \"\"\"\n",
    "üöÄ PR√ìXIMOS PASOS RECOMENDADOS:\n",
    "\n",
    "1. **Mejoras al Modelo Actual**\n",
    "   ‚Ä¢ Probar XGBoost y LightGBM para mejor rendimiento\n",
    "   ‚Ä¢ Implementar stacking/blending de modelos\n",
    "   ‚Ä¢ Agregar m√°s caracter√≠sticas (datos externos, POIs, etc.)\n",
    "   \n",
    "2. **Validaci√≥n Adicional**\n",
    "   ‚Ä¢ Validaci√≥n temporal (train en datos antiguos, test en recientes)\n",
    "   ‚Ä¢ An√°lisis de errores por segmento (geograf√≠a, rango de precio)\n",
    "   ‚Ä¢ Prueba A/B contra m√©todo actual\n",
    "   \n",
    "3. **Preparaci√≥n para Producci√≥n**\n",
    "   ‚Ä¢ Containerizar modelo con Docker\n",
    "   ‚Ä¢ Crear API REST con FastAPI\n",
    "   ‚Ä¢ Implementar monitoreo de drift\n",
    "   ‚Ä¢ Establecer pipeline de reentrenamiento\n",
    "   \n",
    "4. **Consideraciones de Negocio**\n",
    "   ‚Ä¢ Estimar ROI de la implementaci√≥n\n",
    "   ‚Ä¢ Planificar capacitaci√≥n de usuarios\n",
    "   ‚Ä¢ Definir SLAs y m√©tricas de monitoreo\n",
    "   ‚Ä¢ Documentar limitaciones y casos de uso\n",
    "\n",
    "5. **Aspectos √âticos y de Gobernanza**\n",
    "   ‚Ä¢ Auditar sesgos en predicciones\n",
    "   ‚Ä¢ Implementar explicabilidad (SHAP values)\n",
    "   ‚Ä¢ Establecer proceso de actualizaci√≥n de datos\n",
    "   ‚Ä¢ Cumplir con regulaciones de privacidad\n",
    "\"\"\"\n",
    "\n",
    "print(next_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo y el pipeline\n",
    "model_path = Path(\"models\")\n",
    "model_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(best_model, model_path / \"best_random_forest.pkl\")\n",
    "joblib.dump(full_pipeline, model_path / \"preprocessing_pipeline.pkl\")\n",
    "\n",
    "# Guardar metadatos\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'model_type': 'RandomForestRegressor',\n",
    "    'training_date': pd.Timestamp.now().isoformat(),\n",
    "    'metrics': {\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'test_mae': float(test_mae),\n",
    "        'test_r2': float(test_r2)\n",
    "    },\n",
    "    'features': feature_names,\n",
    "    'hyperparameters': random_search.best_params_\n",
    "}\n",
    "\n",
    "with open(model_path / \"model_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"üíæ Modelo guardado exitosamente en ./models/\")\n",
    "print(f\"   ‚Ä¢ Modelo: best_random_forest.pkl\")\n",
    "print(f\"   ‚Ä¢ Pipeline: preprocessing_pipeline.pkl\")\n",
    "print(f\"   ‚Ä¢ Metadata: model_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Material Adicional y Referencias\n",
    "\n",
    "### Referencias Bibliogr√°ficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = \"\"\"\n",
    "üìö REFERENCIAS Y LECTURAS RECOMENDADAS:\n",
    "\n",
    "Libros Fundamentales:\n",
    "‚Ä¢ G√©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn and TensorFlow (2nd ed.)\n",
    "‚Ä¢ Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning\n",
    "‚Ä¢ Kuhn, M., & Johnson, K. (2019). Feature Engineering and Selection\n",
    "\n",
    "Metodolog√≠a y Procesos:\n",
    "‚Ä¢ Chapman, P. et al. (2000). CRISP-DM 1.0: Step-by-step data mining guide\n",
    "‚Ä¢ Provost, F., & Fawcett, T. (2013). Data Science for Business\n",
    "\n",
    "An√°lisis Exploratorio:\n",
    "‚Ä¢ Tukey, J. W. (1977). Exploratory Data Analysis\n",
    "‚Ä¢ Cleveland, W. S. (1993). Visualizing Data\n",
    "‚Ä¢ Wickham, H., & Grolemund, G. (2017). R for Data Science\n",
    "\n",
    "Recursos Online:\n",
    "‚Ä¢ Documentaci√≥n scikit-learn: https://scikit-learn.org/stable/\n",
    "‚Ä¢ Kaggle Learn: https://www.kaggle.com/learn\n",
    "‚Ä¢ Google ML Crash Course: https://developers.google.com/machine-learning/crash-course\n",
    "\n",
    "Papers Relevantes:\n",
    "‚Ä¢ Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32\n",
    "‚Ä¢ Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System\n",
    "\"\"\"\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios Propuestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercises = \"\"\"\n",
    "üèãÔ∏è EJERCICIOS PARA PR√ÅCTICA ADICIONAL:\n",
    "\n",
    "1. **Ingenier√≠a de Caracter√≠sticas Avanzada**\n",
    "   ‚Ä¢ Crear caracter√≠sticas polin√≥micas de grado 2\n",
    "   ‚Ä¢ Implementar target encoding para ocean_proximity\n",
    "   ‚Ä¢ Agregar clustering espacial (K-means en lat/long)\n",
    "\n",
    "2. **Optimizaci√≥n de Hiperpar√°metros**\n",
    "   ‚Ä¢ Usar Optuna o Hyperopt para optimizaci√≥n bayesiana\n",
    "   ‚Ä¢ Implementar early stopping en Gradient Boosting\n",
    "   ‚Ä¢ Probar diferentes m√©tricas de optimizaci√≥n\n",
    "\n",
    "3. **Validaci√≥n Robusta**\n",
    "   ‚Ä¢ Implementar validaci√≥n cruzada anidada\n",
    "   ‚Ä¢ Crear conjunto de validaci√≥n temporal\n",
    "   ‚Ä¢ Analizar estabilidad del modelo con bootstrap\n",
    "\n",
    "4. **Interpretabilidad**\n",
    "   ‚Ä¢ Calcular SHAP values para explicar predicciones\n",
    "   ‚Ä¢ Crear Partial Dependence Plots\n",
    "   ‚Ä¢ Implementar LIME para casos individuales\n",
    "\n",
    "5. **Despliegue**\n",
    "   ‚Ä¢ Crear API REST con Flask/FastAPI\n",
    "   ‚Ä¢ Dockerizar la aplicaci√≥n\n",
    "   ‚Ä¢ Implementar pruebas unitarias\n",
    "\n",
    "6. **Monitoreo**\n",
    "   ‚Ä¢ Detectar data drift con pruebas estad√≠sticas\n",
    "   ‚Ä¢ Implementar alertas de degradaci√≥n de modelo\n",
    "   ‚Ä¢ Crear dashboard de monitoreo con Streamlit\n",
    "\"\"\"\n",
    "\n",
    "print(exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ ¬°Felicitaciones!\n",
    "\n",
    "Has completado exitosamente un proyecto integral de Machine Learning siguiendo las mejores pr√°cticas de la industria. \n",
    "\n",
    "**Recuerda los principios clave:**\n",
    "- üìä **EDA exhaustivo** antes de modelar\n",
    "- üîÑ **Iteraci√≥n constante** siguiendo CRISP-DM  \n",
    "- üõ°Ô∏è **Validaci√≥n rigurosa** para evitar overfitting\n",
    "- üìù **Documentaci√≥n clara** para reproducibilidad\n",
    "- üéØ **Enfoque en el negocio**, no solo en m√©tricas t√©cnicas\n",
    "\n",
    "¬°Ahora est√°s listo para aplicar estos conocimientos en proyectos reales! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fin del Notebook**\n",
    "\n",
    "*√öltima actualizaci√≥n: 2025*  \n",
    "*Autor: Adaptado y mejorado para el curso de Machine Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
