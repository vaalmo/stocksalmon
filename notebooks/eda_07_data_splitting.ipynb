{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d59484",
   "metadata": {},
   "source": [
    "#### Data splitting\n",
    "\n",
    "Acá se concatenan los tres datasets (`chessData.csv`, `random_evals.csv`, `tactic_evals.csv`) y se dividen en:\n",
    "- `training_data` (70%)\n",
    "- `validation_data` (20%)\n",
    "- `testing_data` (10%)\n",
    "\n",
    "Nota: Esta división solo se ejecuta una vez para generar los datasets persistentes. Después de guardar los archivos, se pueden cargar directamente sin repetir la división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7ad54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets...\n",
      "  chessData.csv: 12,958,035 filas\n",
      "  random_evals.csv: 1,000,273 filas\n",
      "  tactic_evals.csv: 2,628,219 filas\n",
      "\n",
      "Dataset concatenado: 16,586,527 filas, 2 columnas\n",
      "Columnas: ['FEN', 'Evaluation']...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path('../data/raw')\n",
    "\n",
    "print('Cargando datasets...')\n",
    "chess_evals = pd.read_csv(DATA_DIR / 'chessData.csv')\n",
    "print(f'  chessData.csv: {len(chess_evals):,} filas')\n",
    "\n",
    "random_evals = pd.read_csv(DATA_DIR / 'random_evals.csv')\n",
    "print(f'  random_evals.csv: {len(random_evals):,} filas')\n",
    "\n",
    "tactic_evals = pd.read_csv(DATA_DIR / 'tactic_evals.csv')\n",
    "print(f'  tactic_evals.csv: {len(tactic_evals):,} filas')\n",
    "\n",
    "raw_data = pd.concat([chess_evals, random_evals, tactic_evals], join='inner', ignore_index=True)\n",
    "print(f'\\nDataset concatenado: {len(raw_data):,} filas, {len(raw_data.columns)} columnas')\n",
    "print(f'Columnas: {list(raw_data.columns[:10])}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302c60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "División completada:\n",
      "  training_data:   11,610,568 filas (70.0%)\n",
      "  validation_data: 3,317,307 filas (20.0%)\n",
      "  testing_data:    1,658,652 filas (10.0%)\n",
      "  Total:           16,586,527 filas\n"
     ]
    }
   ],
   "source": [
    "# Divide los datasets en train (70%), validation (20%), test (10%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# primer split: 70% train, 30% temp (val + test)\n",
    "training_data, temp_data = train_test_split(\n",
    "    raw_data, \n",
    "    test_size=0.30, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# segundo split: dividir temp_data en 20% val y 10% test (2/3 y 1/3 de temp)\n",
    "validation_data, testing_data = train_test_split(\n",
    "    temp_data, \n",
    "    test_size=0.333333,  # 10% del total (1/3 de 30%)\n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print('\\nDivisión completada:')\n",
    "print(f'  training_data:   {len(training_data):,} filas ({len(training_data)/len(raw_data)*100:.1f}%)')\n",
    "print(f'  validation_data: {len(validation_data):,} filas ({len(validation_data)/len(raw_data)*100:.1f}%)')\n",
    "print(f'  testing_data:    {len(testing_data):,} filas ({len(testing_data)/len(raw_data)*100:.1f}%)')\n",
    "print(f'  Total:           {len(training_data) + len(validation_data) + len(testing_data):,} filas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e158228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando datasets divididos...\n",
      "  ✓ training_data.parquet guardado (11,610,568 filas)\n",
      "  ✓ validation_data.parquet guardado (3,317,307 filas)\n",
      "  ✓ testing_data.parquet guardado (1,658,652 filas)\n",
      "\n",
      " Todos los datasets modificados guardados en: C:\\Users\\USUARIO\\Documents\\Code\\stocksalmon\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SPLIT_DIR = Path('../data/processed')\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('\\nGuardando datasets divididos...')\n",
    "\n",
    "# se guardan en formato parquet para eficiencia\n",
    "training_data.to_parquet(SPLIT_DIR / 'training_data.parquet', index=False)\n",
    "print(f'  ✓ training_data.parquet guardado ({len(training_data):,} filas)')\n",
    "\n",
    "validation_data.to_parquet(SPLIT_DIR / 'validation_data.parquet', index=False)\n",
    "print(f'  ✓ validation_data.parquet guardado ({len(validation_data):,} filas)')\n",
    "\n",
    "testing_data.to_parquet(SPLIT_DIR / 'testing_data.parquet', index=False)\n",
    "print(f'  ✓ testing_data.parquet guardado ({len(testing_data):,} filas)')\n",
    "\n",
    "print(f'\\n Todos los datasets modificados guardados en: {SPLIT_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3af29b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cargados:\n",
      "  training_data:   11,610,568 filas\n",
      "  validation_data: 3,317,307 filas\n",
      "  testing_data:    1,658,652 filas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# para cargar los datasets modificados en los otros notebooks\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "SPLIT_DIR = Path('../data/processed')\n",
    "\n",
    "training_data = pd.read_parquet(SPLIT_DIR / 'training_data.parquet')\n",
    "validation_data = pd.read_parquet(SPLIT_DIR / 'validation_data.parquet')\n",
    "testing_data = pd.read_parquet(SPLIT_DIR / 'testing_data.parquet')\n",
    "\n",
    "print('Datasets cargados:')\n",
    "print(f'  training_data:   {len(training_data):,} filas')\n",
    "print(f'  validation_data: {len(validation_data):,} filas')\n",
    "print(f'  testing_data:    {len(testing_data):,} filas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25001bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas: 16,586,527\n",
      "  Train:      11,610,568 (70.0%)\n",
      "  Validation: 3,317,307 (20.0%)\n",
      "  Test:       1,658,652 (10.0%)\n",
      "\n",
      "Columnas en común: 2\n",
      "\n",
      "Primeras 3 filas de training_data:\n",
      "                                                 FEN Evaluation\n",
      "0  r4rk1/pp1qnppp/1n2p1b1/2RpP1B1/1P1P2PN/1Q5P/P3...        +34\n",
      "1  r4rk1/1ppbqppp/p1n1p3/3n4/P1QP4/5NP1/1P1NPPBP/...          0\n",
      "2  r1q2r2/ppp2B1k/2bp1Qnp/8/4P3/5N1P/PPP3P1/3RR1K...       -312\n",
      "\n",
      "Primeras 3 filas de validation_data:\n",
      "                                                 FEN Evaluation\n",
      "0  5rk1/2qnppbp/r2p2p1/1NpP4/P3P3/R4P2/1P4PP/2BQK...        +15\n",
      "1              8/2p2kp1/3p4/3P4/6K1/8/8/8 w - - 1 57      -4795\n",
      "2  1k1r4/ppq2ppp/2p1pn2/4P3/2P5/2Q3N1/PPK2PPP/4R3...          0\n",
      "\n",
      "Primeras 3 filas de testing_data:\n",
      "                                                 FEN Evaluation\n",
      "0  8/4Np1k/3p1Prp/p1p1bRp1/P7/5RPP/1P5K/4r3 b - -...       -310\n",
      "1  5rr1/1p3p1p/5k2/1P2pP2/8/2P3PP/3Q1P1K/8 b - - ...       +544\n",
      "2  r4rk1/p1p3pp/2nqpn2/1Qbpp3/4P3/2PP1N2/PP1N1PPP...       +236\n"
     ]
    }
   ],
   "source": [
    "# verificación de los splits\n",
    "\n",
    "total = len(training_data) + len(validation_data) + len(testing_data)\n",
    "print(f'Total de filas: {total:,}')\n",
    "print(f'  Train:      {len(training_data):,} ({len(training_data)/total*100:.1f}%)')\n",
    "print(f'  Validation: {len(validation_data):,} ({len(validation_data)/total*100:.1f}%)')\n",
    "print(f'  Test:       {len(testing_data):,} ({len(testing_data)/total*100:.1f}%)')\n",
    "\n",
    "print(f'\\nColumnas en común: {len(training_data.columns)}')\n",
    "\n",
    "print('\\nPrimeras 3 filas de training_data:')\n",
    "print(training_data.head(3))\n",
    "print('\\nPrimeras 3 filas de validation_data:')\n",
    "print(validation_data.head(3))\n",
    "print('\\nPrimeras 3 filas de testing_data:')\n",
    "print(testing_data.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
